{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c306cbd7-f740-45f6-b531-f8dfa0da2b50",
   "metadata": {},
   "source": [
    "# Gestion du cache\n",
    "\n",
    "Le principe d'un cache est de conserver les r√©sultats de requ√™tes pour ne pas avoir √† les recalculer lorsque ces requ√™tes seront √† nouveau re√ßues. Le cache est int√©ressant √† partir du moment o√π des requ√™tes reviennent assez fr√©quemment, sachant qu'en contrepartie, la cache va consommer de la ressource m√©moire, disque et CPU. Il faut que cette consommation de ressources soit plus rentable que la consommation faite par le calcul qu'implique la requ√™te re√ßue, pour que le cache soit consid√©r√© comme une option int√©ressante.\n",
    "\n",
    "Spark propose une fonctionnalit√© de cache au niveau Spark Core sur les RDD et Spark SQL sur les dataframe et les dataset. Dans le cadre de Spark SQL, le cache devient int√©ressant d√®s qu'un dataframe est utilis√© dans plusieurs requ√™tes s√©par√©es. Dans cette situation, l'utilisation du cache pour un dataframe permet au niveau de chacune des requ√™tes qui utilisent ce dataframe de r√©√©valuer √† chaque fois les calculs qui ont conduit √† ce dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75390f1b-21bf-451e-80f5-1833495b1e0c",
   "metadata": {},
   "source": [
    "## Pr√©lude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b225a9d5-a485-48cf-96e5-088892f57d63",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Spark tuning - Cache\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"True\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Access the JVM and import the required Java classes\n",
    "jvm = spark.sparkContext._jvm\n",
    "Level = jvm.org.apache.logging.log4j.Level\n",
    "Configurator = jvm.org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "# Set the root level to OFF\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9113a84-bf24-4458-8afa-af6e2319b486",
   "metadata": {},
   "source": [
    "## Introduction sur le cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8acd6703-9a8e-407d-be66-faaa68ba1444",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- client: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      "\n",
      "+--------+----------------+-------------------+------------+-----+\n",
      "|id      |client          |timestamp          |product     |price|\n",
      "+--------+----------------+-------------------+------------+-----+\n",
      "|98432797|oplTx8h-38G3be4c|2023-02-27T10:34:31|d√©caf√©in√©   |1.4  |\n",
      "|90609564|D7pVaSr2-BpeKGwE|2023-03-02T17:59:04|caf√© cr√®me  |2.5  |\n",
      "|03804834|D7pVaSr2-BpeKGwE|2022-12-19T10:50:17|double caf√© |2.6  |\n",
      "|23879812|t_CUBr6tyTQxGj2X|2023-02-03T16:29:14|d√©caf√©in√©   |1.4  |\n",
      "|73975031|TX7wC0pTqCRlCOhi|2023-04-20T09:43:28|caf√© allong√©|1.4  |\n",
      "|15380364|oplTx8h-38G3be4c|2023-01-25T16:38:28|expresso    |1.1  |\n",
      "|25138740|oplTx8h-38G3be4c|2023-05-01T17:37:46|noisette    |1.5  |\n",
      "|57293732|TX7wC0pTqCRlCOhi|2023-05-04T09:27:31|double caf√© |2.6  |\n",
      "|98462522|H-Mp22FLe99MNhRa|2023-03-24T10:08:22|expresso    |1.1  |\n",
      "|68245745|D7pVaSr2-BpeKGwE|2023-03-17T10:57:30|caf√©        |1.3  |\n",
      "+--------+----------------+-------------------+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders = (spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(\"data/orders.csv\")\n",
    "    .repartition(4))\n",
    "\n",
    "orders.printSchema()\n",
    "orders.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a1109d-f2fd-4936-807c-57aad33b8fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sans cache\n",
    "\n",
    "Nous voulons effectuer des analyses sur la journ√©e du 1er f√©vrier 2023, sans utiliser de cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb3fc4f-8b7d-48bc-8a8c-782603d873bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-------------------+--------------+-----+\n",
      "|id      |client          |timestamp          |product       |price|\n",
      "+--------+----------------+-------------------+--------------+-----+\n",
      "|90763431|t_CUBr6tyTQxGj2X|2023-02-01T17:19:26|chocolat chaud|2.6  |\n",
      "|38930443|XztHU0aeUckvR7AC|2023-02-01T13:17:59|caf√© cr√®me    |2.5  |\n",
      "|27370329|iQ5y--CyNtHUDL_8|2023-02-01T10:37:08|chocolat chaud|2.6  |\n",
      "|69412681|JBoCs7rWb_jEs87W|2023-02-01T09:57:30|expresso      |1.1  |\n",
      "|24134740|JBoCs7rWb_jEs87W|2023-02-01T17:58:08|caf√©          |1.3  |\n",
      "|52156742|vAfh79KyDpYeFEMR|2023-02-01T13:02:35|double caf√©   |2.6  |\n",
      "|26805451|D7pVaSr2-BpeKGwE|2023-02-01T13:47:13|double caf√©   |2.6  |\n",
      "|33619683|iQ5y--CyNtHUDL_8|2023-02-01T09:19:38|caf√©          |1.3  |\n",
      "|17099792|H-Mp22FLe99MNhRa|2023-02-01T10:22:44|caf√© allong√©  |1.4  |\n",
      "|77346060|XztHU0aeUckvR7AC|2023-02-01T16:48:48|caf√© cr√®me    |2.5  |\n",
      "+--------+----------------+-------------------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partOfOrders = orders.where(to_date(col(\"timestamp\")) == to_date(lit(\"2023-02-01\")))\n",
    "\n",
    "partOfOrders.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb84b0a-5b3b-4ca2-8a37-0ba5efb0fae7",
   "metadata": {},
   "source": [
    "Nous r√©cup√©rons d'abord le chiffre d'affaires de la journ√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d907dc4d-5cae-46c7-acd4-0cc0d193721d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|total|\n",
      "+-----+\n",
      "|119.2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show revenue - no cache\n",
    "partOfOrders.agg(round(sum(col(\"price\")), 2).alias(\"total\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf5bd4-a426-4590-a08a-81aac76fdd88",
   "metadata": {},
   "source": [
    "Nous allons ensuite analyser la r√©partition du chiffre d'affaires de la journ√©e sur les diff√©rents produits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97b47212-7e41-490f-8590-6a3a7c15d252",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|    product|             total|\n",
      "+-----------+------------------+\n",
      "|   expresso|              23.1|\n",
      "| caf√© cr√®me|              17.5|\n",
      "|double caf√©|15.600000000000001|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show product revenue - no cache\n",
    "(partOfOrders\n",
    ".groupBy(\"product\")\n",
    ".agg(sum(\"price\").alias(\"total\"))\n",
    ".where(\"total >= 15.0\")\n",
    ".orderBy(desc(\"total\"))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169db4e5-a9ce-45c5-a619-4cf8476cd161",
   "metadata": {},
   "source": [
    "Nous r√©cup√©rons enfin la r√©partition des ventes par client sur le caf√© noisette et le total des ventes de la journ√©e sur ce produit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "944085fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|          client|total|\n",
      "+----------------+-----+\n",
      "|vAfh79KyDpYeFEMR|  3.0|\n",
      "|oplTx8h-38G3be4c|  1.5|\n",
      "|JBoCs7rWb_jEs87W|  1.5|\n",
      "|H-Mp22FLe99MNhRa|  1.5|\n",
      "|D7pVaSr2-BpeKGwE|  3.0|\n",
      "|            null| 10.5|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show noisette - no cache\n",
    "(partOfOrders\n",
    ".where(col(\"product\") == \"noisette\")\n",
    ".cube(\"client\")\n",
    ".agg(sum(\"price\").alias(\"total\"))\n",
    ".orderBy(desc(\"client\"))\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057b3b7a-b3ac-41a7-8744-b14ab7d00405",
   "metadata": {},
   "source": [
    "üëÄ **Question** üëÄ\n",
    "\n",
    "Dans toutes ces requ√™tes, lorsque nous regardons le plan d'ex√©cution, nous voyons des DAG relativement √©quivalents. De plus chaque requ√™te se traduit par l'ex√©cution de 3 jobs. Nous le voyons dans ce notebook, au niveau des barres de progression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e38d9a9-2ca5-4d40-a33e-40b3244dac4d",
   "metadata": {},
   "source": [
    "### Avec cache\n",
    "\n",
    "Nous allons maintenant ex√©cuter les m√™mes requ√™tes. Mais cette fois, nous allons mettre en cache le dataframe filtr√© sur la journ√©e 1er f√©vrier 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55da6c9c-4a43-4415-be1f-78e661dde27a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-------------------+--------------+-----+\n",
      "|id      |client          |timestamp          |product       |price|\n",
      "+--------+----------------+-------------------+--------------+-----+\n",
      "|90763431|t_CUBr6tyTQxGj2X|2023-02-01T17:19:26|chocolat chaud|2.6  |\n",
      "|38930443|XztHU0aeUckvR7AC|2023-02-01T13:17:59|caf√© cr√®me    |2.5  |\n",
      "|27370329|iQ5y--CyNtHUDL_8|2023-02-01T10:37:08|chocolat chaud|2.6  |\n",
      "|69412681|JBoCs7rWb_jEs87W|2023-02-01T09:57:30|expresso      |1.1  |\n",
      "|24134740|JBoCs7rWb_jEs87W|2023-02-01T17:58:08|caf√©          |1.3  |\n",
      "|52156742|vAfh79KyDpYeFEMR|2023-02-01T13:02:35|double caf√©   |2.6  |\n",
      "|26805451|D7pVaSr2-BpeKGwE|2023-02-01T13:47:13|double caf√©   |2.6  |\n",
      "|33619683|iQ5y--CyNtHUDL_8|2023-02-01T09:19:38|caf√©          |1.3  |\n",
      "|17099792|H-Mp22FLe99MNhRa|2023-02-01T10:22:44|caf√© allong√©  |1.4  |\n",
      "|77346060|XztHU0aeUckvR7AC|2023-02-01T16:48:48|caf√© cr√®me    |2.5  |\n",
      "+--------+----------------+-------------------+--------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partOfOrders = orders.where(to_date(\"timestamp\") == to_date(lit(\"2023-02-01\"))).cache()\n",
    "\n",
    "partOfOrders.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dcbca5-c329-4ded-940d-980377afceca",
   "metadata": {
    "tags": []
   },
   "source": [
    "üë∑ Ex√©cutez les requ√™tes ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9273c87-89d4-40f7-811c-71b8a6ed6ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|total|\n",
      "+-----+\n",
      "|119.2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show revenue - cache\n",
    "(\n",
    "    partOfOrders\n",
    "        .agg(round(sum(\"price\"), 2).alias(\"total\"))\n",
    "        .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a503bf02-6547-4da5-ad1c-6d43dd21b68f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|    product|             total|\n",
      "+-----------+------------------+\n",
      "|   expresso|              23.1|\n",
      "| caf√© cr√®me|              17.5|\n",
      "|double caf√©|15.600000000000001|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show product revenue - cache\n",
    "(\n",
    "  partOfOrders\n",
    "    .groupBy(\"product\")\n",
    "    .agg(sum(\"price\").alias(\"total\"))\n",
    "    .where(col(\"total\") >= 15.0)\n",
    "    .orderBy(desc(\"total\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1fc4667-8b17-4a3c-8b61-7dedf9f1aed7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|          client|total|\n",
      "+----------------+-----+\n",
      "|vAfh79KyDpYeFEMR|  3.0|\n",
      "|oplTx8h-38G3be4c|  1.5|\n",
      "|JBoCs7rWb_jEs87W|  1.5|\n",
      "|H-Mp22FLe99MNhRa|  1.5|\n",
      "|D7pVaSr2-BpeKGwE|  3.0|\n",
      "|            null| 10.5|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show noisette - cache\n",
    "(\n",
    "  partOfOrders\n",
    "    .where(col(\"product\") == \"noisette\")\n",
    "    .cube(\"client\")\n",
    "    .agg(sum(\"price\").alias(\"total\"))\n",
    "    .orderBy(desc(\"client\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b160e7-5e17-4af8-bce1-6f92a6f3465a",
   "metadata": {},
   "source": [
    "üëÄ **Ce qu'il faut voir** üëÄ\n",
    "\n",
    "La diff√©rence, qui appara√Æt ici, est que l'ex√©cution des requ√™tes d'analyse n√©cessite cette 2 jobs au lieu de 3. Ce qui permet un gain en performance sur l'ex√©cution de ces requ√™tes.\n",
    "\n",
    "En regardant de plus pr√®s le plan d'ex√©cution de ces requ√™tes, nous remarquons que celui-ci est un plus grand. Dans le cadre de la mise en cache, Spark a ajout√© les √©tapes :\n",
    "\n",
    "* **InMemoryRelation** : mise en cache des donn√©es du dataframe\n",
    "* **InMemoryTableScan** : parcours des donn√©es du dataframe depuis le cache\n",
    "\n",
    "Ces √©tapes proviennent de `partOfOrders`.\n",
    "\n",
    "=> üë∑ Ex√©cutez la cellule suivante pour voir le plan d'ex√©cution li√© √† `partOfOrders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "521c5c60-53ee-4152-afe6-ba9ea2f202ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [id#17, client#18, timestamp#19, product#20, price#21]\n",
      "   +- InMemoryRelation [id#17, client#18, timestamp#19, product#20, price#21], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- Exchange RoundRobinPartitioning(4), REPARTITION_BY_NUM, [plan_id=1031]\n",
      "            +- *(1) Filter (isnotnull(timestamp#19) AND (cast(timestamp#19 as date) = 2023-02-01))\n",
      "               +- FileScan csv [id#17,client#18,timestamp#19,product#20,price#21] Batched: false, DataFilters: [isnotnull(timestamp#19), (cast(timestamp#19 as date) = 2023-02-01)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/04_advanced/data/orders.csv], PartitionFilters: [], PushedFilters: [IsNotNull(timestamp)], ReadSchema: struct<id:string,client:string,timestamp:string,product:string,price:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partOfOrders.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8912a685-ffd9-43df-8e2b-f1a60e55d624",
   "metadata": {},
   "source": [
    "En fait, dans les requ√™tes d'analyse, toutes les √©tapes pr√©c√©dentes _InMemoryTableScan_ ne sont pas ex√©cut√©es.\n",
    "\n",
    "Si vous allez dans Spark UI, dans l'onglet _Storage_ vous allez trouver un tableau avec une seule entr√©e, contenant les informations ci-dessous :\n",
    "\n",
    "* Storage Level: Disk Memory Deserialized 1x Replicated\n",
    "* Cached Partitions: 4\n",
    "* Fraction Cached: 100%\n",
    "* Size in Memory: 8.6 KiB\n",
    "* Size on Disk: 0.0 B\n",
    "\n",
    "Cette ligne unique indique les informations sur les donn√©es en cache li√©es √† `partOfOrders`.\n",
    "\n",
    "_Storage Level_ indique le type de stockage utilis√©. Ici, nous avons un stockage en m√©moire d√©s√©rialis√© (il s'agit uniquement d'objets Java stock√©s dans la _heap_ de la JVM) et sur disque, avec un unique r√©pliqua (il n'y a pas de r√©plication sur d'autres executor).\n",
    "\n",
    "Nous avons aussi la quantit√© de m√©moire utilis√©e et la quantit√© de donn√©es stock√©e sur disque.\n",
    "\n",
    "Si vous cliquez sur la ligne du tableau dans Spark UI, vous obtenez plus d'informations sur nos donn√©es en cache. Nous avons notamment la r√©partition des donn√©es en cache par partition avec les executor associ√©s. Et nous avons aussi la m√©moire restante pour le cache.\n",
    "\n",
    "Il est possible de supprimer le cache √† la main en utilisant la m√©thode `.unpersist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "018f0af4-fd6c-4ec7-b7ad-08bc0c31b66b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, client: string, timestamp: string, product: string, price: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partOfOrders.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3506677-751b-4279-91e2-2e10792e0149",
   "metadata": {},
   "source": [
    "Si vous retournez dans l'onglet _Storage_ de Spark UI, vous verrez que le tableau aura disparu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f99d29-5a6b-4c5a-a463-89bd850cb242",
   "metadata": {},
   "source": [
    "### Fonctionnement\n",
    "\n",
    "Le cache Spark SQL est un cache LRU (_Least Recently Used_). Ainsi, lorsque l'espace pour le cache manque, toutes les entr√©es les moins utilis√©es sont retir√©es du cache pour laisser de la place. Lors du prochain acc√®s aux donn√©es retir√©es, Spark recalculera ces donn√©es pour les r√©int√©grer dans le cache, en ayant au pr√©alable retir√© les donn√©es les moins utilis√©es, si le cache est √† nouveau plein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19b8692-88c5-49ce-9b7a-3465d82bd037",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Les diff√©rents niveaux de cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c17a2e-f4d2-426b-8052-861bbda1a87a",
   "metadata": {},
   "source": [
    "### CacheManager\n",
    "\n",
    "Spark SQL b√©n√©ficie d'un CacheManager qui centralise la gestion des caches.\n",
    "\n",
    "Il est possible de vider compl√®tement le cache √† travers son interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7890e15f-d69c-4714-8626-f7ec73dd7fea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_manager = spark._jsparkSession.sharedState().cacheManager()\n",
    "\n",
    "cache_manager.clearCache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84718cc1-c518-4ee3-91a0-a09328dd6c43",
   "metadata": {},
   "source": [
    "Voyons le fonctionnement du CacheManager. Pour cela, nous allons mettre en cache 2 requ√™tes identiques, mais r√©clamant une strat√©gie du stockage du cache diff√©rente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbf49b4c-347c-48c7-a89a-6a06a7a99bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='98432797', client='oplTx8h-38G3be4c', timestamp='2023-02-27T10:34:31', product='d√©caf√©in√©', price='1.4')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_default = spark.read.option(\"header\", \"true\").csv(\"data/orders.csv\").repartition(4).cache()\n",
    "orders_default.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75402a40-2a5f-4d74-955f-0dcb49b12e37",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='98432797', client='oplTx8h-38G3be4c', timestamp='2023-02-27T10:34:31', product='d√©caf√©in√©', price='1.4')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "orders_memory = spark.read.option(\"header\", \"true\").csv(\"data/orders.csv\").repartition(4).persist(StorageLevel.MEMORY_ONLY)\n",
    "orders_default.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3109411-0fb6-4bbd-8fc2-a87d73b0f0db",
   "metadata": {},
   "source": [
    "üëÄ **Ce qu'il faut voir** üëÄ\n",
    "\n",
    "Si nous regardons dans Spark UI, bien que nous ayons produits deux dataframe, nous ne verrons qu'un seul cache dans l'onglet _Storage_.\n",
    "\n",
    "Plus exactement, la recherche du cache se fait selon l'op√©ration suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46f517e9-790b-408d-8549-8a885703ca48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _jdf accesses the underlying Java DataFrame from the PySpark DataFrame\n",
    "orders_default._jdf.queryExecution().logical().sameResult(orders_memory._jdf.queryExecution().logical())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c85ff4-5f8a-4908-ad39-0e8e565ad362",
   "metadata": {},
   "source": [
    "Ce qui veut dire que ce qui est retenu, c'est plus exactement la similarit√© des r√©sultats obtenus par les dataframes et non pas sur l'√©galit√© entre les plans d'ex√©cution des dataframes.\n",
    "\n",
    "La difficult√© est que tout ceci est d√©termin√© avant l'ex√©cution de la requ√™te. Il est dans ce cas parfois difficile de d√©terminer si deux plans d'ex√©cution produiront des r√©sultats √©quivalents. Spark ne sait pas toujours le d√©terminer. Par contre, Spark va capable de d√©terminer si deux plans d'ex√©cution vont donner des r√©sultats diff√©rents.\n",
    "\n",
    "Pour retrouver des donn√©es en cache, Spark SQL passe par la m√©thode `.lookupCachedData()` du CacheManager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bd00245-367d-4b7c-a16b-7d2479797e2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache_orders_default.plan sameResult cache_orders_memory.plan: True\n",
      "cache_orders_default: StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "cache_orders_memory: StorageLevel(disk, memory, deserialized, 1 replicas)\n"
     ]
    }
   ],
   "source": [
    "class TestUniqueCache:\n",
    "    def __init__(self):\n",
    "        self.cacheManager = spark._jsparkSession.sharedState().cacheManager()\n",
    "        self.cache_orders_default = self.cacheManager.lookupCachedData(orders_default._jdf).get()\n",
    "        self.cache_orders_memory = self.cacheManager.lookupCachedData(orders_memory._jdf).get()\n",
    "\n",
    "    def run(self):\n",
    "        same_result = self.cache_orders_default.plan().sameResult(self.cache_orders_memory.plan())\n",
    "        print(f\"cache_orders_default.plan sameResult cache_orders_memory.plan: {same_result}\")\n",
    "        \n",
    "        storage_level_default = self.cache_orders_default.cachedRepresentation().cacheBuilder().storageLevel()\n",
    "        print(f\"cache_orders_default: {storage_level_default}\")\n",
    "        \n",
    "        storage_level_memory = self.cache_orders_memory.cachedRepresentation().cacheBuilder().storageLevel()\n",
    "        print(f\"cache_orders_memory: {storage_level_memory}\")\n",
    "\n",
    "TestUniqueCache().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d887edf-9707-48d8-a5dd-dd45fe3e89be",
   "metadata": {},
   "source": [
    "D'ailleurs, notre dataframe d'origine est lui aussi index√© dans le cache, m√™me si son plan d'ex√©cution d√©duit n'a pas chang√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a571729f-14a2-4838-9a5e-8c116448cd3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [id#17, client#18, timestamp#19, product#20, price#21]\n",
      "   +- InMemoryRelation [id#17, client#18, timestamp#19, product#20, price#21], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- Exchange RoundRobinPartitioning(4), REPARTITION_BY_NUM, [plan_id=1334]\n",
      "            +- FileScan csv [id#1015,client#1016,timestamp#1017,product#1018,price#1019] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/04_advanced/data/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,client:string,timestamp:string,product:string,price:string>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders.explain()\n",
    "cacheManager = spark._jsparkSession.sharedState().cacheManager()\n",
    "cacheManager.lookupCachedData(orders._jdf).isDefined()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c5af9-b077-423a-9107-f7386fc735b4",
   "metadata": {},
   "source": [
    "Si vous ex√©cutez la cellule ci-dessous et que vous allez dans Spark UI, vous verrez appara√Ætre une √©tape InMemoryRelation et InMemoryTableScan, indiquant que les donn√©es pour cette requ√™te n'ont pas √©t√© r√©cup√©r√©es depuis le fichier, mais depuis le cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb9b89b9-18cb-462c-be7f-cb2710d6f966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='98432797', client='oplTx8h-38G3be4c', timestamp='2023-02-27T10:34:31', product='d√©caf√©in√©', price='1.4')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take from order - before clear cache\n",
    "orders.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb4deee-ec83-4059-8878-d53ab8a0e2d0",
   "metadata": {},
   "source": [
    "Par contre, si nous vidons le cache, la r√©cup√©ration des donn√©es se fait depuis le fichier directement. Dans le plan d'ex√©cution, dans Spark UI, les √©tapes InMemoryRelation et InMemoryTableScan auront disparues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe10d59c-7f2c-4f74-a62c-c2c2f277f9dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [id#17, client#18, timestamp#19, product#20, price#21]\n",
      "   +- InMemoryRelation [id#17, client#18, timestamp#19, product#20, price#21], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- Exchange RoundRobinPartitioning(4), REPARTITION_BY_NUM, [plan_id=1334]\n",
      "            +- FileScan csv [id#1015,client#1016,timestamp#1017,product#1018,price#1019] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/jovyan/work/04_advanced/data/orders.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:string,client:string,timestamp:string,product:string,price:string>\n",
      "\n",
      "\n",
      "orders defined ? False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id='98432797', client='oplTx8h-38G3be4c', timestamp='2023-02-27T10:34:31', product='d√©caf√©in√©', price='1.4')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cacheManager.clearCache()\n",
    "\n",
    "orders.explain()\n",
    "print(f\"orders defined ? {cacheManager.lookupCachedData(orders._jdf).isDefined()}\")\n",
    "\n",
    "# take from order - after clear cache\n",
    "orders.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad552ac-2d65-4e81-b5e3-62d5812195a9",
   "metadata": {},
   "source": [
    "### Stockage des caches\n",
    "\n",
    "Voici la liste des strat√©gies de stockage propos√©e par Spark.\n",
    "\n",
    "* **NONE** : pas de persistance (emp√™che le CacheManager de persister par la suite un type de requ√™te)\n",
    "* **DISK_ONLY** : persistance sur le disque local\n",
    "* **DISK_ONLY_2** : persistance sur disque r√©pliqu√©e sur 2 n≈ìuds\n",
    "* **DISK_ONLY_3** : persistance sur disque r√©pliqu√©e sur 3 n≈ìuds\n",
    "* **MEMORY_ONLY** : persistance en m√©moire\n",
    "* **MEMORY_ONLY_2** : persistance en m√©moire sur 2 n≈ìuds\n",
    "* **MEMORY_ONLY_SER** : persistance s√©rialis√©e en m√©moire\n",
    "* **MEMORY_ONLY_SER_2** : persistance s√©rialis√©e en m√©moire sur 2 n≈ìuds\n",
    "* **MEMORY_AND_DISK** : persistance en m√©moire et sur disque\n",
    "* **MEMORY_AND_DISK_2** : persistance en m√©moire et sur disque r√©pliqu√©e sur 2 n≈ìuds\n",
    "* **MEMORY_AND_DISK_SER** : persistance s√©rialis√©e en m√©moire et sur disque\n",
    "* **MEMORY_AND_DISK_SER_2** : persistance s√©rialis√©e en m√©moire et sur disque r√©pliqu√©e sur 2 n≈ìuds\n",
    "* **OFF_HEAP** : persistance en m√©moire _off heap_ (_exp√©riemntal_)\n",
    "\n",
    "La s√©rialisation propos√©e ici utilise un encodage binaire propre √† Spark et qui reconnu comme efficace. Il permet de gagner de l'espace en m√©moire, en demandant un effort suppl√©mentaire sur le CPU.\n",
    "\n",
    "Le stockage sur disque est bien √©videmment plus co√ªteux que le stockage en m√©moire. Il faut dans ce cas que le recalcul du dataframe soit plus lent que l'acc√®s disque pour r√©cup√©rer les donn√©es.\n",
    "\n",
    "La r√©plication est int√©ressante si le recalcul du dataframe est extr√™mement lent et que l'on souhaite donner la possibilit√© √† Spark de reprendre les traitements en cours sur un autre executor, en cas de panne partielle.\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "üë∑ Ci-dessous, apr√®s avoir vid√© le cache, v√©rifier dans Spark UI l'effet du stockage `NONE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c007a2f7-57de-4821-b49a-b71e735c8167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cacheManager.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10df3660-4c07-4792-a9ce-58392afcce04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='56391935', client='JBoCs7rWb_jEs87W', timestamp='2023-02-28T10:45:14', product='noisette', price='1.5')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StorageLevel.NONE\n",
    "orders.where(to_date(\"timestamp\") == to_date(lit(\"2023-02-28\"))).persist(StorageLevel.NONE).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1aa43f-3f65-4194-80f1-668a0ebea587",
   "metadata": {},
   "source": [
    "üë∑ Ex√©cutez les cellules ci-dessous et observez les diff√©rences. A noter que [certaines m√©thodes de s√©rialisation ne sont pas disponibles en PySpark](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence) (ie: MEMORY_ONLY_SER / MEMORY_AND_DISK_SER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2897eb76-e06b-4688-9411-f76a7cd43b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_only = [Row(id='49405607', client='H-Mp22FLe99MNhRa', timestamp='2023-03-01T16:57:01', product='caf√© cr√®me', price='2.5')]\n"
     ]
    }
   ],
   "source": [
    "#StorageLevel.MEMORY_ONLY\n",
    "memory_only = orders.where(to_date(\"timestamp\") == to_date(lit(\"2023-03-01\"))).persist(StorageLevel.MEMORY_ONLY).take(1)\n",
    "print(f\"memory_only = {memory_only}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c33e8778-5746-49f4-a5aa-f42443e192e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disk_only = [Row(id='49405607', client='H-Mp22FLe99MNhRa', timestamp='2023-03-01T16:57:01', product='caf√© cr√®me', price='2.5')]\n"
     ]
    }
   ],
   "source": [
    "#StorageLevel.DISK_ONLY\n",
    "disk_only = orders.where(to_date(\"timestamp\") == to_date(lit(\"2023-03-01\"))).persist(StorageLevel.DISK_ONLY).take(1)\n",
    "print(f\"disk_only = {disk_only}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f7a3fef1-d4d6-454b-99e8-827ced704729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_and_disk = [Row(id='49405607', client='H-Mp22FLe99MNhRa', timestamp='2023-03-01T16:57:01', product='caf√© cr√®me', price='2.5')]\n"
     ]
    }
   ],
   "source": [
    "#StorageLevel.MEMORY_AND_DISK\n",
    "memory_and_disk = orders.where(to_date(\"timestamp\") == to_date(lit(\"2023-03-01\"))).persist(StorageLevel.MEMORY_AND_DISK).take(1)\n",
    "print(f\"memory_and_disk = {memory_and_disk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7697b09-9f7b-4105-a0b3-94208666a756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "off_heap = [Row(id='49405607', client='H-Mp22FLe99MNhRa', timestamp='2023-03-01T16:57:01', product='caf√© cr√®me', price='2.5')]\n"
     ]
    }
   ],
   "source": [
    "#experimental\n",
    "#StorageLevel.OFF_HEAP\n",
    "off_heap = orders.where(to_date(\"timestamp\") == to_date(lit(\"2023-03-01\"))).persist(StorageLevel.OFF_HEAP).take(1)\n",
    "print(f\"off_heap = {off_heap}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277dfc38-0f5b-41ea-bded-5bb74d5f487c",
   "metadata": {},
   "source": [
    "ü§î **Question** ü§î\n",
    "\n",
    "V√©rifiez dans Spark UI la strat√©gie de stockage qui semble la meilleure. Appuyez-vous pour cela sur les statistiques de latence et de consommation d'espace.\n",
    "\n",
    "----\n",
    "\n",
    "Nous allons maintenant voir l'effet d'une strat√©gie de stockage impliquant une r√©plication, sachant que nous n'avons qu'un seul executor (ie. le driver).\n",
    "\n",
    "Apr√®s avoir vid√© le cache et lanc√© la requ√™te, regardez dans Spark UI, dans l'onglet _Storage_, si un espace de stockage a √©t√© cr√©√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0effc384-4b93-4322-88db-90e79c15ce0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='16899776', client='iQ5y--CyNtHUDL_8', timestamp='2023-03-07T09:28:32', product='noisette', price='1.5')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cacheManager.clearCache()\n",
    "#StorageLevel.DISK_ONLY_2\n",
    "orders.where(to_date(\"timestamp\") == to_date(lit(\"2023-03-07\"))).persist(StorageLevel.DISK_ONLY_2).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8292a-0ec5-41f2-8dfb-b88cf0c6c8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
