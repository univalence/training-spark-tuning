{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fc79ce7-6e1e-4f18-87e5-0efcbe21f987",
   "metadata": {},
   "source": [
    "# Broadcast\n",
    "\n",
    "Nous allons voir ici un concept utiliser avec Spark Core et utilisé sous une autre forme avec Spark SQL. Il s'agit des variables _broadcast_.\n",
    "\n",
    "Nous sommes dans une situation où nous avons récupéré une dataset et nous voulons croiser ses données avec celle d'un référentiel. Par exemple, nous avons récupéré des données de commandes client. Sauf que dans le champ client, nous n'avons que des identifiants et pas l'identité du client. Cette relation entre identité et identifiant du client est fourni dans un autre fichier. On pourrait charger ce fichier dans un RDD, mais il est possible que ce fichier des identifiants soit suffisamment petit pour tenir en mémoire et qu'il peut s'avérer plus intéressant de copier ses données dans chaque exécuteur.\n",
    "\n",
    "C'est dans ce cas que les variables _broadcast_ peuvent être utilisées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29244044-b8d6-495e-a9ea-520031795d56",
   "metadata": {},
   "source": [
    "## Prélude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee08235-a954-4b02-a2eb-f4c117e13750",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"Spark tuning - broadcast\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"True\")\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Access the JVM and import the required Java classes\n",
    "jvm = spark.sparkContext._jvm\n",
    "Level = jvm.org.apache.logging.log4j.Level\n",
    "Configurator = jvm.org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "# Set the root level to OFF\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c611e5-678f-463c-b79a-1f29d636cef8",
   "metadata": {},
   "source": [
    "## Chargement d'un fichier de mapping\n",
    "\n",
    "Dans le code ci-dessous, nous allons charger un fichier contenant des correspondances entre identifiant client et nom de client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea7703c-0274-405e-b2d3-790aba82b23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mapping_filename = \"data/client-mapping.csv\"\n",
    "\n",
    "mapping = {}\n",
    "with open(mapping_filename, 'r') as file:\n",
    "    # Skip header\n",
    "    next(file)\n",
    "    \n",
    "    for line in file:\n",
    "        fields = line.strip().split(\",\")\n",
    "        mapping[fields[0]] = fields[1]\n",
    "\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43dead-71a8-498d-9822-bc90ac920655",
   "metadata": {},
   "source": [
    "## Chargement des commandes clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c3500-aef8-4964-a476-3d52e9d8bb39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "\n",
    "rawData = spark.sparkContext.textFile(\"data/orders.csv\", 4)\n",
    "header = rawData.first()\n",
    "data = rawData.filter(lambda line: line != header)\n",
    "\n",
    "def Order(id, clientId, timestamp, product, price):\n",
    "    return Row(id=id, clientId=clientId, timestamp=timestamp, product=product, price=price)\n",
    "\n",
    "def to_local_date_time(s: str) -> datetime:\n",
    "    return datetime.strptime(s, '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "def line_to_order(line: str) -> Row:\n",
    "    fields = line.split(\",\")\n",
    "    return Order(\n",
    "        id=fields[0],\n",
    "        clientId=fields[1],\n",
    "        timestamp=to_local_date_time(fields[2]),\n",
    "        product=fields[3],\n",
    "        price=float(fields[4])\n",
    "    )\n",
    "\n",
    "# Assuming data is an RDD\n",
    "orders = data.map(line_to_order)\n",
    "\n",
    "spark.createDataFrame(orders).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ac5ba-0c11-4665-95e5-4515a0f05d24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Broadcast\n",
    "\n",
    "Une variable _broadcast_ se crée à partir du SparkContext, en utilisant la méthode `.broadcast()` en passant en paramètre la valeur à diffuser sur les exécuteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f9bee-7cea-4797-b9ef-e83161f42b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "broadcast_mapping = spark.sparkContext.broadcast(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5d08d-fd21-4eb8-b5a6-8d9c43d11df5",
   "metadata": {},
   "source": [
    "La récupération de la valeur associée à une variable _broadcast_ se fait en appelant la méthode `.value`.\n",
    "\n",
    "Comme notre table de correspondance `broadcastMapping` représente une collection de type `Map` et que nous souhaitons récupérer le nom d'un client par rapport à son identifiant (`order.clientId`), s'il est présent, nous allons utiliser la méthode `.getOrElse(key, default)` pour récupérer le nom du client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b41822-50a7-4095-bea0-cc6b880396b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_order(order):\n",
    "  order_dict = order.asDict()\n",
    "  order_dict['clientId'] = ???\n",
    "  return Row(**order_dict)\n",
    "\n",
    "mappedOrders = orders.map(map_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84c92f3-b390-4005-9811-64deb17bcebf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Nous obtenons maintenant cet affichage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb07c3-f12e-429e-aa60-3f6d78547cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(mappedOrders).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42af5a-3d3f-45ce-be71-77e1c0eef062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
