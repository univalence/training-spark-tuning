{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617242c4-f978-4e21-b194-c43b6434db97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Fonction d'ordre supérieur\n",
    "\n",
    "Les fonctions d'ordre supérieur dans Spark SQL sont des fonctions qui prennent d'autres fonctions comme arguments ou qui retournent des fonctions. Ces fonctions permettent d'effectuer des opérations complexes sur des données structurées telles que des tableaux ou des structures dans Spark SQL.\n",
    "\n",
    "À partir de la version 2.4, Spark SQL a introduit plusieurs fonctions d'ordre supérieur pour travailler avec des données complexes. Elles ne sont disponibles avec l'API Python que depuis la [version 3.1 de Spark](https://issues.apache.org/jira/browse/SPARK-30681). Voici quelques exemples de fonctions d'ordre supérieur couramment utilisées dans Spark SQL :\n",
    "\n",
    "* **transform** : Applique une fonction donnée à chaque élément d'un tableau et retourne un nouveau tableau avec les résultats.<br />Syntaxe : `transform(array, function)`.\n",
    "* **filter** : Retourne un nouveau tableau contenant les éléments qui satisfont la condition spécifiée par la fonction donnée.<br />Syntaxe : `filter(array, function)`.\n",
    "* **exists** : Vérifie si au moins un élément d'un tableau satisfait la condition spécifiée par la fonction donnée.<br />Syntaxe : `exists(array, function)`.\n",
    "* **forall** : Vérifie si tous les éléments d'un tableau satisfont la condition spécifiée par la fonction donnée.<br />Syntaxe : `forall(array, function)`.\n",
    "* **aggregate** : Agrège les éléments d'un tableau à l'aide d'une fonction d'agrégation et d'une valeur initiale.<br />Syntaxe : `aggregate(array, initial_value, merge_function[, finish_function])`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32428cf8-b4d6-4701-9c53-c11af78affe7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Préambule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7dfa8-24a3-4426-81ab-3138506b1d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark SQL - Fonction d'ordre supérieur\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"True\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access the JVM and import the required Java classes\n",
    "jvm = spark.sparkContext._jvm\n",
    "Level = jvm.org.apache.logging.log4j.Level\n",
    "Configurator = jvm.org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "# Set the root level to OFF\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ad563-d9d0-4d2c-b99f-6ad1513bdbf2",
   "metadata": {},
   "source": [
    "## Chargement\n",
    "\n",
    "Nous allons revenir sur les commandes clients dans une cafétéria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4f237-e002-4ae9-9220-0f064ed91c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(\"id STRING, client STRING, timestamp TIMESTAMP, product STRING, price DOUBLE\") \\\n",
    "    .csv(\"data/orders.csv\")\n",
    "\n",
    "orders = dataframe.withColumnRenamed(\"client\", \"clientId\")\n",
    "\n",
    "orders.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec1306-c028-4a49-8e5c-896696601055",
   "metadata": {},
   "source": [
    "## Ensemble des prix par jour\n",
    "\n",
    "Nous voulons actuellement récupérer pour chaque jour, la liste des prix des produits vendus. Pour cela, nous allons utiliser la  fonction `collect_list(col)`.\n",
    "\n",
    "La fonction `collect_list()` est en quelque sorte l'inverse de la fonction `explode()` : au lieu de décomposer une colonne de type liste en ligne dans le dataframe, `collect_list()` ressemble des lignes du dataframe pour former une colonne de type liste.\n",
    "\n",
    "ATTENTION !!! La quantité de données collectées dans chaque liste ne devrait pas dépasser ~50 000 éléments. Au-delà de cette valeur, vous risquez de dépasser la capacité mémoire de vos exécuteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e36cc8-18f2-45ce-a862-277af699a99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pricesByDay = orders \\\n",
    "    .groupBy(to_date(col(\"timestamp\")).alias(\"date\")) \\\n",
    "    .agg(collect_list(col(\"price\")).alias(\"prices\"))\n",
    "\n",
    "pricesByDay.show(10, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c8905-c117-40fb-b53c-4480d0049f40",
   "metadata": {},
   "source": [
    "## Somme des prix par jour\n",
    "\n",
    "Utilisez la fonction `aggregate(col, init, f)` pour calculer pour chaque jour le prix total de vente.\n",
    " * `col` est la colonne qui contient la liste à agréger.\n",
    " * `init` est la valeur initiale (ou la valeur à retourner si la liste dans `col` est vide). Cette valeur est de type colonne.\n",
    " * `f` est la fonction lambda d'agrégation. Elle a pour signature `lambda accumulator, value: new_value`. Le premier paramètre de la fonction correspond au résultat intermédiaire, sachant que le premier résultat intermédiaire correspond à `init`. Le second paramètre correspond à une valeur provenant de `col`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144155ef-b8a1-4a8a-b5d2-c92532aea7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = ???\n",
    "\n",
    "result.show(10, False)\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17500d-6993-4316-9604-2611d2dc0ec0",
   "metadata": {},
   "source": [
    "## Somme des prix hauts par jour\n",
    "\n",
    "Nous voulons calculer pour chaque jour le prix total de vente pour les articles de 2 EUR ou plus.\n",
    "\n",
    "Utilisez la fonction `filter(col, f)` pour retirer du calcul les prix de moins de 2 EUR.\n",
    " * `col` est la colonne qui contient la liste à agréger\n",
    " * `f` est la fonction de filtrage. Elle a [deux formes](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.filter.html#pyspark.sql.functions.filter). La plus commune permet de dériver un booléen à partir de chaque valeur provenant de `col` (Signature : `(x: Column) -> Column`). La seconde forme permet d'avoir un index en plus de la valeur x (`(x: Column, i: Column) -> Column` où i est un index commençant à 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1a39e-ea73-479d-8e26-59a4de66e081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val result = ???\n",
    "\n",
    "result.show(limit=10, True)\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753f8e2-5528-4ec2-8050-a6a8d201cb2e",
   "metadata": {},
   "source": [
    "Refaites l'exercice, mais en utilisant cette fois une requête SQL.\n",
    "\n",
    "Vous aurez besoin de :\n",
    " * `CAST(col AS type)` : converti le type d'une colonne.\n",
    " * `FILTER(col, f)` : filtre des valeurs dans `col` selon la fonction `f`, où `f` est une fonction de la forme `col -> condition_sur_col`.\n",
    " * `AGGREGATE(col, init, f)` : agrège les données d'une colonne selon `f`, où `f` est une fonction de la forme `(résultat, col) -> agrégation_de_résultat_et_col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784baba-b688-4899-a33c-13d7f2e40568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  date,\n",
    "  ??? AS total\n",
    "FROM (\n",
    "  SELECT\n",
    "    to_date(timestamp) AS date,\n",
    "    collect_list(price) AS price\n",
    "  FROM orders\n",
    "  GROUP BY date\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "result.show(10, False)\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a88507-dde3-40ed-87a2-75737687ece5",
   "metadata": {},
   "source": [
    "## Prix moyen par jour\n",
    "\n",
    "Pour chaque journée, nous voulons calculer le prix moyen. Malheureusement, Spark ne fournit pas de fonction qui permet de calculer une moyenne sur une liste :/\n",
    "\n",
    "Néanmoins, sans avoir à recourir à une UDF, avec l'aide de la fonction `aggregate()`, nous pouvons calculer une telle moyenne. Pour cela, nous devons :\n",
    " 1. D'un côté compter le nombre de prix et de l'autre calculer la somme des prix.\n",
    " 2. Faire la division entre le somme des prix et le décompte pour avoir la moyenne.\n",
    "\n",
    "Il y a deux façons de faire la première étape : soit utiliser 2 fois `aggregate()` pour compter et faire la somme en parallèle, soit utiliser `aggregate()` une seule fois avec une sous-structure qui stocke le décompte et la somme en même temps.\n",
    "\n",
    "À vous de voir :)\n",
    "\n",
    "Vous aurez potentiellement besoin de :\n",
    " * la fonction `struct(column_1, column_2, ...)` : permet de créer une sous structure.\n",
    " * la méthode `.cast(type)` sur une colonne : permet de convertir le type d'une colonne selon le [type passé en paramètre](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html). \n",
    " * la syntaxe `<column>(\"field_name\")` sur une colonne : permet d'accéder au champ `\"field_name\"` d'une colonne, si celle-ci contient une sous-structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd766915-2ef0-4e82-b6b4-994b1a193176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "result = ???\n",
    "\n",
    "result.show(10, True)\n",
    "result.explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
