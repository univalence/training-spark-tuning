{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639a668c-285b-4fc6-97ae-f6bbe1fe88bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Manipulation de fichier\n",
    "\n",
    "SparkSQL est capable de lire et écrire des données depuis ou vers des fichiers de différents formats. Ces fichiers peuvent être des fichiers simples, des fichiers compressés, des fichiers partitionnés, des fichiers partitionnés sur HDFS.\n",
    "\n",
    "Dans ce notebook, nous allons voir comment se comporte SparkSQL avec les fichiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2ac66-e228-4e0d-8afd-94ed51cb292e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Préambule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e849459-ac53-4b30-9e48-fe2463873d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.3.2`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.3.2`\n",
    "import $ivy.`org.slf4j:slf4j-reload4j:2.0.6`\n",
    "\n",
    "import org.apache.logging.log4j.Level\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "// Avoid disturbing logs\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730ca2d7-2034-448f-bc66-8dc87928e8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling /home/jovyan/work/internal/spark_helper.scLoading spark-stubs\n",
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: No SLF4J providers were found.\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://77b5e1be794f:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@77361fa\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "// Ce script fournit des fonctions supplémentaires pour rendre l'affichage plus confortable\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$file.$                      , spark_helper._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    // L'appel ci-dessous sert à donner un nom à votre application\n",
    "    // Ce apparaîtra notamment dans la Spark UI\n",
    "    .appName(\"SparkSQL - Fichiers\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Ce script fournit des fonctions supplémentaires pour rendre l'affichage plus confortable\n",
    "import $file.^.internal.spark_helper, spark_helper._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bac0a3-e994-4ad8-b6a7-ce6a020e0b03",
   "metadata": {
    "tags": []
   },
   "source": [
    "Exécutez la cellule ci-dessous à chaque fois que vous souhaitez recommencer les exercices plus bas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca0b96-cc94-457e-97dd-efe0fc28a07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanTarget = {\n",
    "    import java.nio.file.{Files, Path, Paths}\n",
    "\n",
    "    def deleteDirectory(path: Path): Unit = {\n",
    "      if (Files.isDirectory(path)) {\n",
    "        // List the directory contents and delete them recursively\n",
    "        Files.list(path).forEach(deleteDirectory)\n",
    "      }\n",
    "      // Delete the file or directory (if it's a directory, it should be empty by now)\n",
    "      Files.delete(path)\n",
    "    }\n",
    "\n",
    "    val targetDirectory = Paths.get(\"target/\")\n",
    "    deleteDirectory(targetDirectory)\n",
    "    Files.createDirectory(targetDirectory)\n",
    "}\n",
    "\n",
    "cleanTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e3d8b-01b7-46a3-8b4a-9597dcef71eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lecture d'un fichier JSON compressé\n",
    "\n",
    "SparkSQL est capable de gérer naturellement les fichiers compressés, sur différents algorithmes de compression (gzip, bzip2, snappy, lz4, zstd...). La compression permet de gagner de l'espace de stockage et d'augmenter le débit du transfert de données. Il sera en général plus efficace sur les fichiers textes que sur les fichiers binaires. La compression demande un peu plus d'utilisation CPU.\n",
    "\n",
    "Avec la commande shell ci-dessous, nous pouvons voir qu'il existe un JSON compressé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4275e-3228-479a-8b33-2648c83f7c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shell(\"ls -algFh data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c546bb-b147-45b1-b719-c597f291718c",
   "metadata": {},
   "source": [
    "Utilisez Spark pour charger le fichier JSON compressé.\n",
    "\n",
    "La méthode `.repartition()` plus bas permet de forcer la redistribution des données dans plusieurs partitions. La valeur passée en paramètre correspond au nombre de partitions souhaité. Cette valeur est limitée par le nombre de Core/CPU disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a4563-2df2-4edf-85a5-aee6a0cb1ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val rawDataframe = ???\n",
    "\n",
    "val dataframe =\n",
    "  rawDataframe\n",
    "    .repartition(4)\n",
    "\n",
    "dataframe.showHTML(limit=10, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e2ec5-024c-4c9c-b89f-bd60da57c8b7",
   "metadata": {},
   "source": [
    "Nous allons voir combien de partitions sont associées au dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c0153-7bb8-4493-b4bd-21c9bde91898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c78c21-2c98-4cea-bf4c-c8913800391f",
   "metadata": {},
   "source": [
    "## Sauvegarde dans des fichiers Parquet\n",
    "\n",
    "Nous allons maintenant tester différents algorithmes de compression.\n",
    "\n",
    "La fonction ci-dessous va permettre de visualiser pour chaque algorithme ses performances et termes de capacité de compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff4e06-cdfa-46ec-abc6-88e93227f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSaveParquet(dataframe: DataFrame, alg: String): Unit = {\n",
    "  val file = s\"orders-$alg.parquet\"\n",
    "  dataframe.repartition(8).write.option(\"compression\", alg).parquet(s\"target/$file\")\n",
    "  shell(s\"ls -algFh target/$file\")\n",
    "  shell(s\"du -h target/$file\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d060ee6-03a1-47f3-8ee2-125e78cfc563",
   "metadata": {
    "tags": []
   },
   "source": [
    "Dans chaque cas ci-dessous, regardez et comparez les différents résultats obtenus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907de5b5-7a91-4fb6-ba22-1bfad90bd859",
   "metadata": {},
   "source": [
    "### Pas de compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2db99-d7cd-47dc-a338-ea8e9f9bf6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testSaveParquet(\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9bb81-712b-4e3c-a3bd-0c195fb9985d",
   "metadata": {},
   "source": [
    "### Snappy compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aef6bf-01c6-4980-a579-7f7ff53bc70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testSaveParquet(\"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31c241-d722-4e32-8fb9-49b52cf2bcf2",
   "metadata": {},
   "source": [
    "### GZip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d10d8d-60e3-47c3-bcdd-e18c39ef4e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testSaveParquet(\"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328d480-8e7a-4550-9009-218f28dc4fb8",
   "metadata": {},
   "source": [
    "### BZip2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64317a10-c7c2-4ab3-9491-e5c5bec8eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSaveParquet(\"bzip2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644cdfd-0edf-4ff2-9a3b-4b2d7ad90738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
