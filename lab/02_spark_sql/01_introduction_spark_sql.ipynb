{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f1118e-c68e-44a0-b930-0ffa55e86538",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction à Spark SQL\n",
    "\n",
    "Spark SQL est un module d'Apache Spark, qui facilite la mise en place de traitement sur des données à haute volumétrie :\n",
    " * **structurées** : la donnée est stockée sous un format standardisé (CSV, JSON, Avro, Parquet...) et répond à une structure partagée (ie. schéma) répondant à un besoin technique ou métier\n",
    " * **semi-structurées** : la donnée est stockée sous un format standardisé, mais sa structure interne n'est pas connue par avance.\n",
    "\n",
    "Spark SQL offre une interface pour interagir avec les données via le langage SQL, ainsi que des fonctionnalités pour la lecture et l'écriture de données dans divers formats. Spark SQL facilite l'intégration entre le traitement des données relationnelles et le traitement distribué à grande échelle en utilisant les DataFrames et les Datasets, deux structures de données immuables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19baddc0-a3fe-468b-b2cc-958e9c435b9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Préambule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37318b58-9804-4a27-a375-5fffe4ed62a7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.4.1`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.4.1`\n",
    "import $ivy.`org.slf4j:slf4j-reload4j:2.0.6`\n",
    "\n",
    "import org.apache.logging.log4j.Level\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "// Avoid disturbing logs\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611e135-c2c4-4056-a972-c98e16e7095c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .progress(enable = true, keep = true, useBars = false)\n",
    "    .master(\"local[*]\")\n",
    "    // L'appel ci-dessous sert à donner un nom à votre application\n",
    "    // Ce apparaîtra notamment dans la Spark UI\n",
    "    .appName(\"SparkSQL - Introduction\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Ce script fournit des fonctions supplémentaires pour rendre l'affichage plus confortable\n",
    "import $file.^.internal.spark_helper\n",
    "import spark_helper.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44468d-2a3d-4c88-a659-d679831f76ef",
   "metadata": {},
   "source": [
    "**Note** : la variable `spark` définie ci-dessus représente la session SparkSQL.\n",
    "\n",
    "La ligne `import spark.implicits._` permet de récupérer des codecs permettant de gérer les données sérialisées, ainsi que la possibilité d'utiliser la notation `$\"<nom-colonne>\"` pour référencer des colonnes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca8a53-5f83-49eb-aa1c-6cfca43b1d27",
   "metadata": {},
   "source": [
    "## Lecture d'un fichier avec Spark SQL\n",
    "\n",
    "Nous allons récupérer le fichier `orders.csv` et réaliser des analyses sur ce fichier.\n",
    "\n",
    "Commençons par afficher un extrait de son contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d149965-e818-401a-a0df-e215bfcf491a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cat data/orders.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500bdcd5-ca8b-4927-b5a6-18ef5e14bee8",
   "metadata": {},
   "source": [
    "### Lecture : première approche\n",
    "La récupération du contenu d'un fichier avec Spark SQL va s'avérer beaucoup plus simple qu'avec Spark Core, car Spark SQL est fourni avec un ensemble de codec pour gérer les formats CSV, JSON, texte, binaire, Avro, Parquet, ORC.\n",
    "\n",
    "Nous allons récupérer le contenu du fichier `data/orders.csv`. Dans le cadre de SparkSQL, la récupération d'un fichier commence par l'appel à `spark.read`, chaîner éventuellement avec une succession d'option (ie. `.option(\"<option_name>\", <option_value>)`). Le chaînage se termine par l'appel d'une méthode dont le nom représente le format de donnée.\n",
    "\n",
    "Ci-dessous nous utilisons l'options `header`, qui permet d'indiquer que la première du fichier contient les en-têtes de colonne. Ces en-têtes sont ensuite utilisées pour nommer les colonnes parmis les données récupérées par SparkSQL.\n",
    " \n",
    "D'autres options sont disponibles pour, par exemple, préciser un séparateur de colonne différent, le format de date utilisé, l'utilisation d'un algorithme de compression (`none`, `bzip2`, `gzip`, `lz4`, `snappy`, `deflate`)... Consultez la [documentation à ce sujet](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option) pour la liste exhaustive.\n",
    "\n",
    "#### Dataframe\n",
    "\n",
    "Le résultat de ce chaînage d'appel est un dataframe. Un dataframe, comme les RDD, est une abstraction de données distribuées dans Apache Spark. Contrairement aux RDD, les dataframes sont spécifiquement conçus pour faciliter le traitement et l'analyse de données structurées et semi-structurées. Ils sont inspirés des DataFrames de R et de la bibliothèque Pandas du langage Python. Cette abstraction offre une API haut niveau pour travailler avec des données tabulaires dans un contexte distribué, en se basant sur des opérations propres au langage SQL.\n",
    "\n",
    "Note : la méthode `.showHTML()` et le _magic hook_ `%%data` permettent aussi d'afficher un extrait d'un dataframe. Normalement, pour afficher un dataframe, il faut appeler dessus la méthode `.show()`, fournie par Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeedbca-52a8-45c9-9a45-0593fbba7b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data limit=10\n",
    "\n",
    "val dataframe: DataFrame =\n",
    "  spark.read\n",
    "    // indique que le fichier contient une ligne d'en-tête qui servira\n",
    "    // pour nommer les champs\n",
    "    .option(\"header\", true)\n",
    "    // lecture du fichier au format CSV\n",
    "    .csv(\"data/orders.csv\")\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e19787-09b2-47e2-9563-e4795d7db428",
   "metadata": {},
   "source": [
    "Juste à titre d'exemple, voici ce que donne comme affichage la méthode `.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33e8b0-87ab-4b50-934d-4e34bc63b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648929d-da5b-4cff-a216-bd0a1f4003e5",
   "metadata": {},
   "source": [
    "Nous allons afficher le schéma associé, afin de comprendre ce qui a été récupérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed58211-cbfa-4689-af8d-2d6098cf269e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fbe10b-d2a4-418f-bd74-b2c30fc8ac57",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Tout d'abord, pour récupérer le contenu d'un fichier SparkSQL a exécuté 2 jobs, que vous pouvez voir dans Spark UI. Le premier job (csv) a permis de récupérer les en-têtes de colonne. Le second job (showHTML) a permis de récupérer juste les lignes nécessaires pour l'affichage.\n",
    "\n",
    "Nous pouvons voir un nouvel onglet dans l'affichage de Spark UI. Il s'agit de \"SQL / DataFrame\". Celui-ci permet de voir l'ensemble des requêtes SparkSQL que vous avez exécutés, avec leur job associé, leur plan d'exécution sous forme de DAG et comme le nombre de lignes récupérer, le nombre de fichiers traités.\n",
    "\n",
    "Nous voyons que lorsque SparkSQL récupère des données SQL, par défaut toutes les données sont interprétées comme des chaînes de caractères. Sachant que nous avons des dates et des prix. Ceci ne nous convient pas. Nous allons voir si nous pouvons faire mieux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1936fec-e4dc-4470-a88c-2db6f30772b4",
   "metadata": {},
   "source": [
    "### Lecture : deuxième approche\n",
    "\n",
    "Nous allons utiliser une autre options\n",
    " * `inferSchema` (`true`/`false`) : demande à SparkSQL de réaliser une pré-analyse des données du fichier pour déterminer le type associé à chaque colonne.\n",
    "\n",
    "Utilisez cette option dans le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c0edb-9968-4c69-963f-696d694e4069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data limit=10\n",
    "\n",
    "val dataframe: DataFrame =\n",
    "  spark.read\n",
    "    // indique que le fichier contient une ligne d'en-tête qui servira\n",
    "    // pour nommer les champs\n",
    "    .option(\"header\", true)\n",
    "    // lecture du fichier au format CSV\n",
    "    .csv(\"data/orders.csv\")\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116c4fd-ad87-44a9-978a-ed0c3a9a3517",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Dans la Spark UI, vous pouvez voir un nouvel onglet dans la barre du haut intitulé \"SQL / DataFrame\". En cliquant dessus, vous verrez apparaître les requêtes exécutées par Spark SQL. Si vous cliquez sur une requête, vous verrez un diagramme représentant le plan d'exécution et dans la partie \"Details\" une représentation textuelle du plan d'exécution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbc2e0-b274-4b8b-b4d5-d79fabad1969",
   "metadata": {},
   "source": [
    "Affichons le schéma de notre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99870fe4-99ad-4c27-a869-b0905c5a4650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae43160-2d64-4a69-b5fc-ed91ef3a66e0",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Avec l'instruction `dataframe.printSchema`, nous pouvons voir que Spark a réussi à déterminer le schéma des données du fichier. Ce qui inclut le fait de déterminer le nom des colonnes et de déterminer le type des colonnes (grâce à l'option `inferSchema` pour ce dernier). Cependant, l'option `inferSchema` a deux problèmes :\n",
    "\n",
    " * Il nécessite une lecture supplémentaire du fichier (sur un extrait). Si vous regardez les barres de progression ci-dessus et dans Spark UI, vous verrez deux étapes de lecture CSV.\n",
    " * Il peut se tromper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592403e-0275-45a7-971e-6663888a81bb",
   "metadata": {},
   "source": [
    "### Lecture : troisième approche\n",
    "Nous allons maintenant relire le fichier CSV, mais cette fois en fournissant directement un schéma entré à la main.\n",
    "\n",
    "Cette fois, vous n'utiliserez pas l'option `inferSchema`. À la place, vous utiliserez la méthode `.schema()`, avant d'appeler `.csv()`, avec le schéma suivant à passer en paramètre :\n",
    "\n",
    "```\n",
    "\"id STRING, client STRING, timestamp TIMESTAMP, product STRING, price DOUBLE\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fafa1-5a6e-4b0c-b27b-131f61d096a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data limit=10\n",
    "\n",
    "val dataframe: DataFrame =\n",
    "  spark.read\n",
    "    // indique que le fichier contient une ligne d'en-tête qui servira\n",
    "    // pour nommer les champs\n",
    "    .option(\"header\", true)\n",
    "    // lecture du fichier au format CSV\n",
    "    .csv(\"data/orders.csv\")\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a289858-9634-41d9-ac61-94534c255b37",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Le fait de fournir un schéma va inciter Spark SQL à ne pas réaliser des analyses préalables ou des vérifications. Nous voyons, en effet, que l'ensemble du process est réduit à un job au lieu de deux ou trois."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4749a5e-bc70-46b9-97a0-cac762b35f07",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Dataframe est une abstraction générique. Il y a certes un schéma associé aux données récupérées, mais les données du dataframe n'est pas associé à un modèle mémoire.\n",
    "\n",
    "Dans le cadre des langages Scala et Java, SparkSQL fournit une autre abstraction : `Dataset[A]`. Cette abstraction permet d'associer des données récupérées avec SparkSQL à un modèle défini en mémoire : typiquement une classe Java ou une case class Scala.\n",
    "\n",
    "Un `Dataset` peut se créer à partir d'une collection ou d'un RDD. Il est possible d'en créer depuis un `DataFrame` en utilisant la méthode `.as[T]`, où `T` représente le type (case classe) que doit le `Dataset`. Veillez à ce que le nom des champs de votre case class corresponde correctement au nom des colonnes du `DataFrame`. N'hésitez pas à renommer les colonnes, si besoin.\n",
    "\n",
    "**Note** : en réalité, un `DataFrame` est alias du type `Dataset[Row]`, où `Row` est une représentation générique d'une ligne de données.\n",
    "\n",
    "Ci-dessous, créez un `Dataset` à partir du dataframe défini plus haut et de la case class Order. N'hésitez pas à utiliser la méthode `.withColumnRenamed(<ancient>, <nouveau>)` pour renommer des colonnes si nécessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd0a10-f7e7-4e92-b0fa-c5ebbe68416c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data limit=10\n",
    "\n",
    "import java.sql.Timestamp\n",
    "\n",
    "case class Order(\n",
    "  id:        String,\n",
    "  clientId:  String,\n",
    "  timestamp: Timestamp,\n",
    "  product:   String,\n",
    "  price:     Double\n",
    ")\n",
    "\n",
    "// Cette est nécessaire ici uniquement, dans le cadre de ce notebook.\n",
    "// Normalement, vous n'avez pas besoins d'y faire appel.\n",
    "spark_helper.sparkExport(this)\n",
    "\n",
    "val orders: Dataset[Order] =\n",
    "  dataframe\n",
    "    .withColumnRenamed(\"client\", \"clientId\")\n",
    "    .as[Order]\n",
    "\n",
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75afb3d-9848-4cb8-b26b-d23940e73991",
   "metadata": {},
   "source": [
    "## Trouvez le produit le plus vendu (ID du produit et quantité totale vendue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbd36b-ed57-428c-9a09-5363ef3a2636",
   "metadata": {},
   "source": [
    "### Première approche : utilisation de l'API SparkSQL\n",
    "\n",
    "Lorsque vous utilisez l'API SparkSQL, vous allez utiliser des méthodes comme `.select()`, `.where()`, `.groupBy()`... Ces méthodes prennent en paramètre des références sur des colonnes.\n",
    "\n",
    "Il y a trois manières de référencer une colonne :\n",
    " * `dataframe(\"nom-colonne\")` : en utilisant un dataframe/dataset défini\n",
    " * `col(\"nom-colonne\")` : en utilisant les fonctions prédéfinies dans `org.apache.spark.sql.functions._`\n",
    " * `$\"nom-colonne\"` (Scala uniquement) : si vous avez importé `spark.implicits._`, où `spark` est la session Spark.\n",
    " \n",
    "Il existe des références spéciales à des colonnes, autrement dit, des colonnes qui n'en sont pas toujours :\n",
    " * `lit(constante)` : permet d'utiliser une constante au niveau d'une colonne.\n",
    " * les fonctions provenant de `org.apache.spark.sql.functions._` fournissent en sortie une référence spéciale à une colonne indépendamment de leur finalité.\n",
    " * `*` : référence toutes les colonnes d'un dataset.\n",
    " * `a.b` : permet d'extraire le champ `b` d'une colonne `a`, lorsque `a` contient une sous-structure.\n",
    " * `a.*` : permet d'extraire tous les champs d'une colonne `a`, lorsque `a` contient une sous-structure.\n",
    " * `a[n]` : permet d'extraire l'élément d'index `n` de la colonne `a`, lorsque `a` contient une liste.\n",
    " \n",
    "L'ensemble des méthodes applicables sont disponibles dans la [Scaladoc de SparkSQL](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e37da0-c7a9-404d-bbbc-e9ebe603205e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// utilisez .groupBy(colonne) pour regroupes les commandes par produit\n",
    "val ordersByProduct = ???\n",
    "\n",
    "// pas d'affichage possible ici :/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1cfe4-c74b-41ba-8028-a657f4baaa54",
   "metadata": {},
   "source": [
    "Maintenant que nous avons regroupé les entités par clé (ie. le même produit), nous allons agréger ces entités.\n",
    "\n",
    "L'agrégation se fait avec la méthode `.agg()`. Cette fonction prend en paramètre une fonction d'agrégation.\n",
    "\n",
    "Par exemple pour compter le nombre d'instances d'une clé :\n",
    "\n",
    "```scala\n",
    "df.groupBy($\"keyCol\").agg(count(lit(1)).as(\"count\"))\n",
    "```\n",
    "\n",
    "Dans ce code, `.as()` permet de donner un nom expoitable à une colonne. Sans cet alias, vous allez vous retrouver avec une colonne qui s'appelle `count(1)`\n",
    "\n",
    "L'ensemble des fonctions applicables dans la méthode `.agg()` sont disponibles dans la section \"Aggregate functions\" de la [Scaladoc de functions](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67117df-84bc-418a-aad7-035dec9baca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "// utilisez .agg() et la fonction count() pour compter le nombre de commandes par produit\n",
    "val countOrdersByProduct = ???\n",
    "\n",
    "countOrdersByProduct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007aabd1-5f03-48a4-9846-9a9f2b116d56",
   "metadata": {},
   "source": [
    "Nous allons maintenant trier les scores des produits, en commençant par le produit le plus vendu. Pour cela, nous allons utiliser la méthode `.orderBy()`, qui permet de trier par rapport à une colonne.\n",
    "\n",
    "Il est possible d'utiliser la méthode `.desc` sur une colonne pour indiquer qu'on souhaite que le tri est décroisant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b4a03-c3fb-4bdb-b149-2a89edcfc9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "// utilisez .orderBy(colonne) pour trier les produits\n",
    "val sortedCountOrdersByProduct = ???\n",
    "\n",
    "sortedCountOrdersByProduct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a861cbd-6207-4bb7-bddc-9153b73ec4eb",
   "metadata": {},
   "source": [
    "### Deuxième approche : utilisation d'une requête SQL\n",
    "\n",
    "Nous allons faire le même exercice, mais cette fois en utilisant une requête SQL.\n",
    "\n",
    "Pour cela nous devons créer une vue (SQL) sur notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37416028-01fc-4cff-b396-f0c35804a9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f81bf2-71bb-4c63-b775-4a7786d6c683",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant écrire la requête SQL en utilisant la vue crée.\n",
    "\n",
    "Complétez la requête ci-dessous, en vous assurant que vous obtenez bien le même résultat que dans le cadre de l'exercice précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e8bac-c2c4-4780-8d0a-9a0c028a1f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT ???\n",
    "FROM ???\n",
    "GROUP BY ???\n",
    "ORDER BY ??? DESC\n",
    "\"\"\").showHTML()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121623aa-0938-45ca-94d6-b8aad394b712",
   "metadata": {},
   "source": [
    "Avec les facilités offertes dans ces notebooks, vous pouvez écrire directement la requête de cette façon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00986ca4-7eed-421d-baf8-53e1adac78b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT ???\n",
    "FROM ???\n",
    "GROUP BY ???\n",
    "ORDER BY ??? DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6bfb9-a1f1-47b3-8cfb-bd7ec36e09d5",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Si vous regardez le plan d'exécution de cette requête et que vous le comparez au dernier plan d'exécution obtenu à travers l'utilisation de l'API Spark SQL, vous remarquerez que ces deux plans d'exécution sont identiques. Ce qui indique bien que les deux approches font exactement la même chose et qu'elles le font avec les mêmes performances.\n",
    "\n",
    "Ainsi, Spark SQL vous donne la possibilité d'utiliser le langage qui vous convient le plus, tout en ayant le même comportement de la part de Spark. Ceci est vrai dans la majorité des cas, si vous vous tenez aux fonctions de base fournies par Spark SQL. C'est moins vrai dès que vous introduisez des éléments personnalisés (eg. UDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf9d8d-faa7-4ffd-a935-2e73d4c93896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
