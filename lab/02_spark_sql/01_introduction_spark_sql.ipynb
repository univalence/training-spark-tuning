{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98f1118e-c68e-44a0-b930-0ffa55e86538",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Introduction à Spark SQL\n",
    "\n",
    "Spark SQL est un module d'Apache Spark, qui facilite la mise en place de traitement sur des données à haute volumétrie :\n",
    " * **structurées** : la donnée est stockée sous un format standardisé (CSV, JSON, Avro, Parquet...) et répond à une structure partagée (ie. schéma) répondant à un besoin technique ou métier\n",
    " * **semi-structurées** : la donnée est stockée sous un format standardisé, mais sa structure interne n'est pas connue par avance.\n",
    "\n",
    "Spark SQL offre une interface pour interagir avec les données via le langage SQL, ainsi que des fonctionnalités pour la lecture et l'écriture de données dans divers formats. Spark SQL facilite l'intégration entre le traitement des données relationnelles et le traitement distribué à grande échelle en utilisant les DataFrames et les Datasets, deux structures de données immuables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19baddc0-a3fe-468b-b2cc-958e9c435b9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Préambule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37318b58-9804-4a27-a375-5fffe4ed62a7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.4.1`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.4.1`\n",
    "import $ivy.`org.slf4j:slf4j-reload4j:2.0.6`\n",
    "\n",
    "import org.apache.logging.log4j.Level\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2611e135-c2c4-4056-a972-c98e16e7095c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .progress(enable = true, keep = true, useBars = false)\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Spark SQL - Introduction\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "import $file.^.internal.spark_helper\n",
    "import spark_helper.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44468d-2a3d-4c88-a659-d679831f76ef",
   "metadata": {},
   "source": [
    "**Note** : la variable `spark` définie ci-dessus représente la session SparkSQL.\n",
    "\n",
    "La ligne `import spark.implicits._` permet de récupérer des codecs permettant de gérer les données sérialisées, ainsi que la possibilité d'utiliser la notation `$\"<nom-colonne>\"` pour référencer des colonnes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ca8a53-5f83-49eb-aa1c-6cfca43b1d27",
   "metadata": {},
   "source": [
    "## Lecture d'un fichier avec Spark SQL\n",
    "\n",
    "Nous allons récupérer le fichier `orders.csv` et réaliser des analyses sur ce fichier.\n",
    "\n",
    "Commençons par afficher un extrait de son contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d149965-e818-401a-a0df-e215bfcf491a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cat data/orders.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500bdcd5-ca8b-4927-b5a6-18ef5e14bee8",
   "metadata": {},
   "source": [
    "### Lecture : première approche\n",
    "La récupération du contenu d'un fichier avec Spark SQL va s'avérer beaucoup plus simple qu'avec Spark Core, car Spark SQL est fourni avec un ensemble de codec pour gérer les formats CSV, JSON, texte, binaire, Avro, Parquet, ORC.\n",
    "\n",
    "Nous allons récupérer le contenu du fichier `data/orders.csv`. Dans le cadre de SparkSQL, la récupération d'un fichier commence par l'appel à `spark.read`, chaîner éventuellement avec une succession d'option (ie. `.option(\"<option_name>\", <option_value>)`). Le chaînage se termine par l'appel d'une méthode dont le nom représente le format de donnée.\n",
    "\n",
    "Ci-dessous nous utilisons l'options `header`, qui permet d'indiquer que la première du fichier contient les en-têtes de colonne. Ces en-têtes sont ensuite utilisées pour nommer les colonnes parmis les données récupérées par SparkSQL.\n",
    " \n",
    "D'autres options sont disponibles pour, par exemple, préciser un séparateur de colonne différent, le format de date utilisé, l'utilisation d'un algorithme de compression (`none`, `bzip2`, `gzip`, `lz4`, `snappy`, `deflate`)... Consultez la [documentation à ce sujet](https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option) pour la liste exhaustive.\n",
    "\n",
    "#### Dataframe\n",
    "\n",
    "Le résultat de ce chaînage d'appel est un dataframe. Un dataframe, comme les RDD, est une abstraction de données distribuées dans Apache Spark. Contrairement aux RDD, les dataframes sont spécifiquement conçus pour faciliter le traitement et l'analyse de données structurées et semi-structurées. Ils sont inspirés des DataFrames de R et de la bibliothèque Pandas du langage Python. Cette abstraction offre une API haut niveau pour travailler avec des données tabulaires dans un contexte distribué, en se basant sur des opérations propres au langage SQL.\n",
    "\n",
    "Note : la méthode `.showHTML()` et le _magic hook_ `%%data` permettent aussi d'afficher un extrait d'un dataframe. Normalement, pour afficher un dataframe, il faut appeler dessus la méthode `.show()`, fournie par Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeedbca-52a8-45c9-9a45-0593fbba7b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data limit=10\n",
    "\n",
    "val dataframe: DataFrame =\n",
    "  spark.read\n",
    "    // indique que le fichier contient une ligne d'en-tête qui servira\n",
    "    // pour nommer les champs\n",
    "    .option(\"header\", true)\n",
    "    // lecture du fichier au format CSV\n",
    "    .csv(\"data/orders.csv\")\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e19787-09b2-47e2-9563-e4795d7db428",
   "metadata": {},
   "source": [
    "Juste à titre d'exemple, voici ce que donne comme affichage la méthode `.show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33e8b0-87ab-4b50-934d-4e34bc63b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648929d-da5b-4cff-a216-bd0a1f4003e5",
   "metadata": {},
   "source": [
    "Nous allons afficher le schéma associé, afin de comprendre ce qui a été récupérer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed58211-cbfa-4689-af8d-2d6098cf269e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fbe10b-d2a4-418f-bd74-b2c30fc8ac57",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Tout d'abord, pour récupérer le contenu d'un fichier SparkSQL a exécuté 2 jobs, que vous pouvez voir dans Spark UI. Le premier job (csv) a permis de récupérer les en-têtes de colonne. Le second job (showHTML) a permis de récupérer juste les lignes nécessaires pour l'affichage.\n",
    "\n",
    "Nous pouvons voir un nouvel onglet dans l'affichage de Spark UI. Il s'agit de \"SQL / DataFrame\". Celui-ci permet de voir l'ensemble des requêtes SparkSQL que vous avez exécutés, avec leur job associé, leur plan d'exécution sous forme de DAG et comme le nombre de lignes récupérer, le nombre de fichiers traités.\n",
    "\n",
    "Nous voyons que lorsque SparkSQL récupère des données SQL, par défaut toutes les données sont interprétées comme des chaînes de caractères. Sachant que nous avons des dates et des prix. Ceci ne nous convient pas. Nous allons voir si nous pouvons faire mieux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1936fec-e4dc-4470-a88c-2db6f30772b4",
   "metadata": {},
   "source": [
    "### Lecture : deuxième approche\n",
    "\n",
    "Nous allons utiliser une autre options\n",
    " * `inferSchema` (`true`/`false`) : demande à SparkSQL de réaliser une pré-analyse des données du fichier pour déterminer le type associé à chaque colonne.\n",
    "\n",
    "Utilisez cette option dans le code ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c0edb-9968-4c69-963f-696d694e4069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data limit=10\n",
    "\n",
    "val dataframe: DataFrame =\n",
    "  spark.read\n",
    "    // indique que le fichier contient une ligne d'en-tête qui servira\n",
    "    // pour nommer les champs\n",
    "    .option(\"header\", true)\n",
    "    // lecture du fichier au format CSV\n",
    "    .csv(\"data/orders.csv\")\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116c4fd-ad87-44a9-978a-ed0c3a9a3517",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Dans la Spark UI, vous pouvez voir un nouvel onglet dans la barre du haut intitulé \"SQL / DataFrame\". En cliquant dessus, vous verrez apparaître les requêtes exécutées par Spark SQL. Si vous cliquez sur une requête, vous verrez un diagramme représentant le plan d'exécution et dans la partie \"Details\" une représentation textuelle du plan d'exécution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbc2e0-b274-4b8b-b4d5-d79fabad1969",
   "metadata": {},
   "source": [
    "Affichons le schéma de notre dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99870fe4-99ad-4c27-a869-b0905c5a4650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae43160-2d64-4a69-b5fc-ed91ef3a66e0",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Avec l'instruction `dataframe.printSchema`, nous pouvons voir que Spark a réussi à déterminer le schéma des données du fichier. Ce qui inclut le fait de déterminer le nom des colonnes et de déterminer le type des colonnes (grâce à l'option `inferSchema` pour ce dernier). Cependant, l'option `inferSchema` a deux problèmes :\n",
    "\n",
    " * Il nécessite une lecture supplémentaire du fichier (sur un extrait). Si vous regardez les barres de progression ci-dessus et dans Spark UI, vous verrez deux étapes de lecture CSV.\n",
    " * Il peut se tromper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592403e-0275-45a7-971e-6663888a81bb",
   "metadata": {},
   "source": [
    "### Lecture : troisième approche\n",
    "Nous allons maintenant relire le fichier CSV, mais cette fois en fournissant directement un schéma entré à la main.\n",
    "\n",
    "Cette fois, vous n'utiliserez pas l'option `inferSchema`. À la place, vous utiliserez la méthode `.schema()`, avant d'appeler `.csv()`, avec le schéma suivant à passer en paramètre :\n",
    "\n",
    "```\n",
    "\"id STRING, client STRING, timestamp TIMESTAMP, product STRING, price DOUBLE\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84fafa1-5a6e-4b0c-b27b-131f61d096a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data limit=10\n",
    "\n",
    "val dataframe: DataFrame =\n",
    "  spark.read\n",
    "    // indique que le fichier contient une ligne d'en-tête qui servira\n",
    "    // pour nommer les champs\n",
    "    .option(\"header\", true)\n",
    "    // lecture du fichier au format CSV\n",
    "    .csv(\"data/orders.csv\")\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a289858-9634-41d9-ac61-94534c255b37",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Le fait de fournir un schéma va inciter Spark SQL à ne pas réaliser des analyses préalables ou des vérifications. Nous voyons, en effet, que l'ensemble du process est réduit à un job au lieu de deux ou trois."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4749a5e-bc70-46b9-97a0-cac762b35f07",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Dataframe est une abstraction générique. Il y a certes un schéma associé aux données récupérées, mais les données du dataframe n'est pas associé à un modèle mémoire.\n",
    "\n",
    "Dans le cadre des langages Scala et Java, SparkSQL fournit une autre abstraction : `Dataset[A]`. Cette abstraction permet d'associer des données récupérées avec SparkSQL à un modèle défini en mémoire : typiquement une classe Java ou une case class Scala.\n",
    "\n",
    "Un `Dataset` peut se créer à partir d'une collection ou d'un RDD. Il est possible d'en créer depuis un `DataFrame` en utilisant la méthode `.as[T]`, où `T` représente le type (case classe) que doit le `Dataset`. Veillez à ce que le nom des champs de votre case class corresponde correctement au nom des colonnes du `DataFrame`. N'hésitez pas à renommer les colonnes, si besoin.\n",
    "\n",
    "**Note** : en réalité, un `DataFrame` est alias du type `Dataset[Row]`, où `Row` est une représentation générique d'une ligne de données.\n",
    "\n",
    "Ci-dessous, créez un `Dataset` à partir du dataframe défini plus haut et de la case class Order. N'hésitez pas à utiliser la méthode `.withColumnRenamed(<ancient>, <nouveau>)` pour renommer des colonnes si nécessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dd0a10-f7e7-4e92-b0fa-c5ebbe68416c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data limit=10\n",
    "\n",
    "import java.sql.Timestamp\n",
    "\n",
    "case class Order(\n",
    "  id:        String,\n",
    "  clientId:  String,\n",
    "  timestamp: Timestamp,\n",
    "  product:   String,\n",
    "  price:     Double\n",
    ")\n",
    "\n",
    "// Cette est nécessaire ici uniquement, dans le cadre de ce notebook.\n",
    "// Normalement, vous n'avez pas besoins d'y faire appel.\n",
    "spark_helper.sparkExport(this)\n",
    "\n",
    "val orders: Dataset[Order] =\n",
    "  dataframe\n",
    "    .withColumnRenamed(\"client\", \"clientId\")\n",
    "    .as[Order]\n",
    "\n",
    "orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75afb3d-9848-4cb8-b26b-d23940e73991",
   "metadata": {},
   "source": [
    "## Trouvez le produit le plus vendu (ID du produit et quantité totale vendue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbd36b-ed57-428c-9a09-5363ef3a2636",
   "metadata": {},
   "source": [
    "### Première approche : utilisation de l'API SparkSQL\n",
    "\n",
    "Lorsque vous utilisez l'API SparkSQL, vous allez utiliser des méthodes comme `.select()`, `.where()`, `.groupBy()`... Ces méthodes prennent en paramètre des références sur des colonnes.\n",
    "\n",
    "Il y a trois manières de référencer une colonne :\n",
    " * `dataframe(\"nom-colonne\")` : en utilisant un dataframe/dataset défini\n",
    " * `col(\"nom-colonne\")` : en utilisant les fonctions prédéfinies dans `org.apache.spark.sql.functions._`\n",
    " * `$\"nom-colonne\"` (Scala uniquement) : si vous avez importé `spark.implicits._`, où `spark` est la session Spark.\n",
    " \n",
    "Il existe des références spéciales à des colonnes, autrement dit, des colonnes qui n'en sont pas toujours :\n",
    " * `lit(constante)` : permet d'utiliser une constante au niveau d'une colonne.\n",
    " * les fonctions provenant de `org.apache.spark.sql.functions._` fournissent en sortie une référence spéciale à une colonne indépendamment de leur finalité.\n",
    " * `*` : référence toutes les colonnes d'un dataset.\n",
    " * `a.b` : permet d'extraire le champ `b` d'une colonne `a`, lorsque `a` contient une sous-structure.\n",
    " * `a.*` : permet d'extraire tous les champs d'une colonne `a`, lorsque `a` contient une sous-structure.\n",
    " * `a[n]` : permet d'extraire l'élément d'index `n` de la colonne `a`, lorsque `a` contient une liste.\n",
    " \n",
    "L'ensemble des méthodes applicables sont disponibles dans la [Scaladoc de SparkSQL](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e37da0-c7a9-404d-bbbc-e9ebe603205e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// utilisez .groupBy(colonne) pour regroupes les commandes par produit\n",
    "val ordersByProduct = ???\n",
    "\n",
    "// pas d'affichage possible ici :/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1cfe4-c74b-41ba-8028-a657f4baaa54",
   "metadata": {},
   "source": [
    "Maintenant que nous avons regroupé les entités par clé (ie. le même produit), nous allons agréger ces entités.\n",
    "\n",
    "L'agrégation se fait avec la méthode `.agg()`. Cette fonction prend en paramètre une fonction d'agrégation.\n",
    "\n",
    "Par exemple pour compter le nombre d'instances d'une clé :\n",
    "\n",
    "```scala\n",
    "df.groupBy($\"keyCol\").agg(count(lit(1)).as(\"count\"))\n",
    "```\n",
    "\n",
    "Dans ce code, `.as()` permet de donner un nom expoitable à une colonne. Sans cet alias, vous allez vous retrouver avec une colonne qui s'appelle `count(1)`\n",
    "\n",
    "L'ensemble des fonctions applicables dans la méthode `.agg()` sont disponibles dans la section \"Aggregate functions\" de la [Scaladoc de functions](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67117df-84bc-418a-aad7-035dec9baca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "// utilisez .agg() et la fonction count() pour compter le nombre de commandes par produit\n",
    "val countOrdersByProduct = ???\n",
    "\n",
    "countOrdersByProduct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007aabd1-5f03-48a4-9846-9a9f2b116d56",
   "metadata": {},
   "source": [
    "Nous allons maintenant trier les scores des produits, en commençant par le produit le plus vendu. Pour cela, nous allons utiliser la méthode `.orderBy()`, qui permet de trier par rapport à une colonne.\n",
    "\n",
    "Il est possible d'utiliser la méthode `.desc` sur une colonne pour indiquer qu'on souhaite que le tri est décroisant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b4a03-c3fb-4bdb-b149-2a89edcfc9d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "// utilisez .orderBy(colonne) pour trier les produits\n",
    "val sortedCountOrdersByProduct = ???\n",
    "\n",
    "sortedCountOrdersByProduct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a861cbd-6207-4bb7-bddc-9153b73ec4eb",
   "metadata": {},
   "source": [
    "### Deuxième approche : utilisation d'une requête SQL\n",
    "\n",
    "Nous allons faire le même exercice, mais cette fois en utilisant une requête SQL.\n",
    "\n",
    "Pour cela nous devons créer une vue (SQL) sur notre dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37416028-01fc-4cff-b396-f0c35804a9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orders.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f81bf2-71bb-4c63-b775-4a7786d6c683",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant écrire la requête SQL en utilisant la vue crée.\n",
    "\n",
    "Complétez la requête ci-dessous, en vous assurant que vous obtenez bien le même résultat que dans le cadre de l'exercice précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e8bac-c2c4-4780-8d0a-9a0c028a1f97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT ???\n",
    "FROM ???\n",
    "GROUP BY ???\n",
    "ORDER BY ??? DESC\n",
    "\"\"\").showHTML()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121623aa-0938-45ca-94d6-b8aad394b712",
   "metadata": {},
   "source": [
    "Avec les facilités offertes dans ces notebooks, vous pouvez écrire directement la requête de cette façon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00986ca4-7eed-421d-baf8-53e1adac78b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT ???\n",
    "FROM ???\n",
    "GROUP BY ???\n",
    "ORDER BY ??? DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6bfb9-a1f1-47b3-8cfb-bd7ec36e09d5",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Si vous regardez le plan d'exécution de cette requête et que vous le comparez au dernier plan d'exécution obtenu à travers l'utilisation de l'API Spark SQL, vous remarquerez que ces deux plans d'exécution sont identiques. Ce qui indique bien que les deux approches font exactement la même chose et qu'elles le font avec les mêmes performances.\n",
    "\n",
    "Ainsi, Spark SQL vous donne la possibilité d'utiliser le langage qui vous convient le plus, tout en ayant le même comportement de la part de Spark. Ceci est vrai dans la majorité des cas, si vous vous tenez aux fonctions de base fournies par Spark SQL. C'est moins vrai dès que vous introduisez des éléments personnalisés (eg. UDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df344081-08d5-4764-b0f7-1b34d525e1c8",
   "metadata": {},
   "source": [
    "## Jointure\n",
    "\n",
    "L'une des fonctionnalités importantes de Spark SQL est sa capacité à réaliser des jointures entre datasets avec beaucoup de flexibilités.\n",
    "\n",
    "La jointure consiste à associer les lignes de 2 datasets en fonctions d'un critère lié à des clés déterminées sur les datasets. Une clé de dataset correspond à l'ensemble des colonnes qui permettent d'identifier uniquement une ou plusieurs lignes d'un dataset.\n",
    "\n",
    "Dans le cas des commandes clients, les clés possibles peuvent être l'identifiant de commande, l'identifiant client (pour regrouper l'ensemble des consommations des clients), le produit (pour avoir l'ensemble des ventes par produit). Les clés peuvent être composées à partir de plusieurs colonnes (le client et le produit permettent d'avoir les consommations des clients par produit). Les clés peuvent être calculées à partir d'une ou plusieurs colonnes (on peut extraire la date du jour à partir du timestamp, ce qui permet d'avoir les consommations par jour).\n",
    "\n",
    "Au niveau de la jointure, nous allons pouvoir exprimer une relation d'égalité entre les clés des 2 datasets (il est possible d'exprimer une relation d'inégalité).\n",
    "\n",
    "```scala\n",
    "dataset1.join(dataset2, dataset1(\"key1\") === dataset2(\"key2\"))\n",
    "```\n",
    "\n",
    "Il ressort de cette requête un nouveau dataset composé à partir des 2 datasets initiaux."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29087635-cd07-4d25-af68-f7885386ce7f",
   "metadata": {},
   "source": [
    "Ici, nous allons convertir l'identifiant des clients pour obtenir leur nom. Un fichier de mapping au format CSV est proposé et contient pour chaque identifiant client, le nom correspondant.\n",
    "\n",
    "Avec la cellule ci-dessous, affichez un extrait du fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f2b46-a865-43d5-b1a3-a45c519bc0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cat data/client-mapping.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df04b0c-d16e-4c77-bfd1-69d7356eec97",
   "metadata": {},
   "source": [
    "Ci-dessous, vous allez charger le fichier `data/client-mapping.csv` dans le dataframe `mapping`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e492b3e-9088-4c19-8eb7-4005dc411f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "val mapping = ???\n",
    "\n",
    "mapping.printSchema()\n",
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699629e8-cbdd-4f37-ab5e-7201fca6dbc3",
   "metadata": {},
   "source": [
    "Partez maintenant du dataset `orders` et appelez dessus la méthode `.join()` pour réaliser une jointure avec le dataframe `mapping`. `.join()` prend 2 paramètres :\n",
    "1. le dataset qui intervient à droite dans la jointure\n",
    "2. la relation entre les clés\n",
    "\n",
    "Pour la relation entre les clés, nous voulons faire correspondre les 2 colonnes `clientId` du côté `orders` et du côté `mapping` (pensez à utiliser un triple égal (`===`) pour représenter cette relation).\n",
    "\n",
    "À la fin, on souhaite faire apparaître uniquement les colonnes \"id\", \"name\", \"timestamp\", \"product\", \"price\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7331e2-d662-481f-8229-554e9c7fe84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%data limit=10\n",
    "\n",
    "orders\n",
    "  .join(???, ???)\n",
    "  .select(\"id\", \"name\", \"timestamp\", \"product\", \"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cbb108-3db7-41df-8053-9a43d9097747",
   "metadata": {},
   "source": [
    "Par défaut, Spark SQL propose un _inner join_, c'est-à-dire que Spark SQL ne va conserver que les lignes sur lesquels il a trouvé une correspondance. Spark SQL permet d'exprimer d'autres formes de jointure comme :\n",
    "* _left outer join_ : toutes lignes sur lesquels une correspondance a été trouvée, ainsi que toutes les lignes du dataset de **gauche** n'ayant pas de correspondance.\n",
    "* _right outer join_ : toutes lignes sur lesquels une correspondance a été trouvée, ainsi que toutes les lignes du dataset de **droite** n'ayant pas de correspondance.\n",
    "* _full outer join_ : toutes les lignes à gauche et à droite ayant ou pas une correspondance.\n",
    "\n",
    "Pour les lignes n'ayant pas de correspondance, les colonnes manquantes sont complétées avec la valeur `null`.\n",
    "\n",
    "Spark SQL propose d'autres forment de jointures et divers algorithmes pour les traiter. Nous verrons plus en détail ces aspects dans la partie optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3eccf-a359-4c85-a88a-d1f3458ec6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
