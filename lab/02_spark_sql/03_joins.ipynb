{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68479a4-719b-444e-89f5-a3743865c26d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Jointure\n",
    "\n",
    "D'une manière générale, le sujet des jointures en big data est un des sujets de ce domaine les plus complexes.\n",
    "\n",
    "Le principe d'une jointure consiste, en effet, à fusionner deux datasets en recherchant dans les deux datasets les lignes qui partagent la même clé. Un algorithme naïf va résoudre une jointure en $O(n^2)$ : \"pour chaque élément du premier dataset, je parcours le second dataset pour retrouver le ou les éléments paratageant la même clé\". Dans un contexte big data, une telle complexité n'est pas envisageable, du fait de la taille des données à traiter et des communications réseau que cela peut engendrer.\n",
    "\n",
    "Il existe heureusement des algorithmes de jointure bien plus efficaces, qui vont consister à utiliser une indexation sur la clé et/ou un tri sur la clé et/ou la diffusion d'un dataset entier sur les différents exécuteurs pour réaliser les opérations de recherche en local.\n",
    "\n",
    "**De préférence, faites l'exercice en binôme.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d4b41-1a01-47bf-9ac2-6db0fb40a8d2",
   "metadata": {},
   "source": [
    "## Préambule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4e136-2975-47dc-8c84-677735b45c09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.3.2`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.3.2`\n",
    "import $ivy.`org.slf4j:slf4j-reload4j:2.0.6`\n",
    "\n",
    "import org.apache.logging.log4j.Level\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "// Avoid disturbing logs\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6ea18-3a5f-4877-932a-c33aacd24b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "import java.net.URL\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    // L'appel ci-dessous sert à donner un nom à votre application\n",
    "    // Ce apparaîtra notamment dans la Spark UI\n",
    "    .appName(\"SparkSQL - Jointure\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Ce script fournit des fonctions supplémentaires pour rendre l'affichage plus confortable\n",
    "import $file.^.internal.spark_helper, spark_helper._\n",
    "\n",
    "println(\">>> Emplacement de Spark UI\")\n",
    "spark.sparkContext.uiWebUrl.foreach(url => println(s\"  * $url\"))\n",
    "spark.sparkContext.uiWebUrl.map(new URL(_).getPort).map(port => s\"http://localhost:$port\").foreach(url => println(s\"  * $url\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a3be6-2f88-413b-b6f5-8da8755c8e3f",
   "metadata": {},
   "source": [
    "## Chargement des datasets\n",
    "\n",
    "Nous allons utiliser dans ce notebook des datasets différents de ce qui a été vu jusque-là. Nous nous mettons dans le cadre d'une application sur téléphone, qui permet à ses utilisateurs de partager sa position avec leurs amis. Pour cela, l'application détecte par géolocalisation le lieu et propose à l'utilisateur de partager le lieu (_checkin_). Une gamification est ajoutée, qui permet à l'utilisateur de gagner des badges selon la fréquence de ses checkins, la distance entre deux chekins, l'utilisateur qui effectue régulièrement les premiers checkins de la journée d'un même lieu devient \"Maire\" de ce lieu...\n",
    "\n",
    "Nous avons deux datasets :\n",
    "\n",
    "* Venues : représente un ensemble de lieux enregistrés qu'il est possible de visiter. Il contient notamment l'identifiant du lieu et ses coordonnées.\n",
    "* Checkins : représente l'ensemble des checkins réalisés par les utilisateurs de l'application. Il contient l'identifiant de l'utilisateur, l'identifiant du lieu et le timestamp du checkin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cc1fd-e291-4ee1-9b1e-326afd9a7cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import java.time.Instant\n",
    "\n",
    "case class Venue(id: String, latitude: Double, longitude: Double, locationType: String, country: String)\n",
    "case class Checkin(userId: String, venueId: String, timestamp: Instant)\n",
    "\n",
    "val venuesFilename   = \"data/threetriangle/venues.txt.gz\"\n",
    "val checkinsFilename = \"data/threetriangle/checkins.txt.gz\"\n",
    "\n",
    "val venues =\n",
    "  spark.read\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .schema(\"id STRING, latitude DOUBLE, longitude DOUBLE, locationType STRING, country STRING\")\n",
    "    .csv(venuesFilename)\n",
    "    .as[Venue]\n",
    "\n",
    "val checkins =\n",
    "  spark.read\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    // Option to correctly interpret timestamp in checkin data\n",
    "    .option(\"timestampFormat\", \"EEE MMM d HH:mm:ss Z yyyy\")\n",
    "    .schema(\"userId STRING, venueId STRING, timestamp TIMESTAMP, tzOffset INT\")\n",
    "    .csv(checkinsFilename)\n",
    "    .as[Checkin]\n",
    "\n",
    "venues.createOrReplaceTempView(\"venues\")\n",
    "checkins.createOrReplaceTempView(\"checkins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb241a-30ba-4c69-a4f1-c9bce997a7c6",
   "metadata": {},
   "source": [
    "## Jointure\n",
    "\n",
    "Nous voulons maintenant retrouver les informations des différents lieux où ont été effectués les checkins. Nous avons donc besoin de réaliser une jointure entre les datasets Checkins et Venues.\n",
    "\n",
    "La méthode `.join()` est assez simple à utiliser. Elle s'applique sur un premier dataframe, puis vous spécifiez en paramètre le dataframe avec lequel vous créez une jointure, en indiquant éventuellement la relation de jointure et le type de jointure (inner, outer, left outer, right outer..., par défaut : inner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda33502-59e6-46d8-bd9f-f73b4ed9e358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.join(venues, checkins(\"venueId\") === venues(\"id\"))\n",
    "data.showHTML(limit=10, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b8855-c80f-42b9-a9b3-00844eec0d6b",
   "metadata": {},
   "source": [
    "Affichons le plan d'exécution de cette requête. Essayer de percevoir les différentes, dont celles qui concernent la jointure (Quelles optimisations sont apportées ? Y a-t-il des échanges de données ?)\n",
    "\n",
    "Note : un plan d'exécution se lit du bas vers le haut pour suivre les différentes étapes de traitement dans l'ordre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e386a-29dd-4148-ba61-c0dc60e2e8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517903ff-1464-43ce-ab8f-3c859f167fa2",
   "metadata": {},
   "source": [
    "Allez dans Spark UI et explorez l'onglet \"SQL / DataFrame\". Spark UI montre un DAG assez complexe pour la requête.\n",
    "\n",
    "**Approche alternative**\n",
    "\n",
    "Voici l'équivalent de notre requête, mais cette fois exprimée en SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc943e7b-25fb-4419-bb07-0a9553a85f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data =\n",
    "  spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  checkins c INNER JOIN venues v ON (v.id = c.venueId)\n",
    "\"\"\")\n",
    "\n",
    "data.showHTML(limit=10, truncate=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d533d-e406-4377-b301-36a6548fae32",
   "metadata": {},
   "source": [
    "Le plan d'exécution de cette requête est-il similaire au plan précédent ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9860e9b-4c23-40ac-849a-758b3de68551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75647c-d147-4cb2-b008-3cd21781f2ac",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Dans le processus de génération du plan d'exécution physique, nous pouvons voir que Spark fini par sélectionner une stratégie de jointure. Cette stratégie se traduit par une phase d'échange (ou plus exactement, de diffusion) de données entre exécuteurs (`BroadcastExchange`). Puis, interviens la phase de jointure basée sur le _hash_ de la clé de jointure (`BroadcastHashJoin`).\n",
    "\n",
    "Nous pouvons voir aussi que Spark se laisse la possibilité au dernier moment de changer de stratégie de jointure (`AdaptiveSparkPlan isFinalPlan=false`), sur la base de données statistiques récupérées pendant l'exécution.\n",
    "\n",
    "Ce type de jointure est similaire à ce que nous avons vu avec les variables broadcast dans le cas des RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637c689-4fb5-4bb3-a548-a57fa1524920",
   "metadata": {},
   "source": [
    "## Stratégie de jointure\n",
    "\n",
    "Spark SQL dispose de différentes stratégies de jointure. Nous venons d'en voir une dans la section précédente.\n",
    "\n",
    "Sans précision dans le code, cette stratégie est choisie selon une heuristique paramétrable liée, notamment, au type de la jointure, la condition sur les clés (équivalence ou non-équivalence), à taille des données ou à d'autres données statistiques.\n",
    "\n",
    "Si vous voulez forcer la stratégie de jointure, vous pouvez le préciser dans le code à travers des _hints_ :\n",
    "\n",
    "```scala\n",
    "  df1.join(df2.hint(\"<Stratégie>\"), ...)\n",
    "```\n",
    "\n",
    "En SQL :\n",
    "\n",
    "```scala\n",
    "  spark.sql(\"\"\"SELECT /*+ <Stratégie> */ ...\"\"\")\n",
    "```\n",
    "\n",
    "Voici les différentes stratégies (et les valeurs à utiliser dans les hints) :\n",
    "\n",
    " * Broadcast Hash Join (BROADCAST / BROADCASTJOIN / MAPJOIN)\n",
    " * Shuffle Sort-Merge Join (MERGE / SHUFFLE_MERGE / MERGEJOIN)\n",
    " * Shuffle Hash Join (SHUFFLE_HASH)\n",
    " * Shuffle-and-Replicate Nested Loop Join (SHUFFLE_REPLICATE_NL)\n",
    "\n",
    "### Broadcast Hash Join\n",
    "\n",
    "Cette stratégie est utilisée dans ces conditions :\n",
    " * La relation de jointure se base sur l'égalité entre les clés (ie. `dataset1(\"key\") === dataset2(\"key\")`).\n",
    " * L'un des datasets est considéré comme étant suffisamment petit.\n",
    " * Tous les types de jointure sont supportés, sauf FULL OUTER JOIN.\n",
    "\n",
    "Dans ce cas, le \"petit\" dataset est transmis (ou diffusé, d'où le terme _broadcast_) sous la forme d'une table de hachage (_hash table_) à l'ensemble des exécuteurs participant à la jointure. La jointure est alors réalisée en local sur chaque nœud.\n",
    "\n",
    "Le seuil indiquant si la stratégie sera utilisée est fixée par le paramètre `spark.sql.autoBroadcastJoinThreshold`. Il est exprimé en octets (valeur par défaut : 10485760 (= 10 MB)). Si l'un des datasets de la jointure à une taille inférieure à ce seuil, il sera diffusé. Si aucun \n",
    "\n",
    "Dans le cadre de la fonctionnalité AQE (_Adaptive Query Execution_), qui permet de pousser Spark à sélectionner une autre startégie sur la base de relevés statistiques, au lieu de la stratégie planifiée initialement, le seuil permettant de passer à la stratégie _Broadcast Hash Join_ est fixé par le paramètre `spark.sql.adaptive.autoBroadcastJoinThreshold` exprimé en octet. Si vous fixez sa valeur à -1, cette fonctionnalité est désactivée.\n",
    "\n",
    "Un autre paramètre intervient : `spark.sql.broadcastTimeout`. Ce paramètre est exprimé en secondes (valeur par défaut : 300 (= 5mn)). Il est détecté des problèmes de communication durant la diffusion des données. Si la diffusion de la table ne peut pas être terminée dans le délai imparti, Spark interrompt l'exécution de la requête et génère une erreur.\n",
    "\n",
    "### Shuffle Sort-Merge Join\n",
    "\n",
    "Cette stratégie est utilisée dans ces conditions :\n",
    " * La relation de jointure se base sur l'égalité entre les clés (ie. `dataset1(\"key\") === dataset2(\"key\")`).\n",
    " * Il est possible de trier les clés.\n",
    " * Tous les types de jointure sont supportés.\n",
    " \n",
    "C'est la stratégie utilisée par défaut (sauf dans le cas ou le paramètre `spark.sql.join.preferSortMergeJoin=false`, dans ce cas, Spark utilisera _Shuffle Hash Join_).\n",
    "\n",
    "Les étapes de jointure sont :\n",
    " 1. Échanges des données entre exécuteurs en fonction de la clé.\n",
    " 2. Tri des données en local en fonction de la clé.\n",
    " 3. Fusion des deux datasets.\n",
    "\n",
    "En général, _Shuffle Sort-Merge Join_ a de meilleures performances sur des données volumineuses.\n",
    "\n",
    "### Shuffle Hash Join\n",
    "\n",
    "Cette stratégie est utilisée dans ces conditions :\n",
    " * La relation de jointure se base sur l'égalité entre les clés (ie. `dataset1(\"key\") === dataset2(\"key\")`).\n",
    " * Il est possible de trier les clés.\n",
    " * Tous les types de jointure sont supportés, sauf FULL OUTER JOIN.\n",
    "\n",
    "Les étapes de jointure sont :\n",
    " 1. Échanges des données entre exécuteurs en fonction de la clé.\n",
    " 2. Dépôt des données transférées en local dans une table de hachage en fonction de la clé.\n",
    " 3. Fusion des deux datasets.\n",
    "\n",
    "_Shuffle Hash Join_ est efficace lorsque la clé utilisée pour la jointure permet un partitionnement équilibré de la donnée.\n",
    "\n",
    "### Shuffle-and-Replicate Nested Loop Join\n",
    "\n",
    "Cette stratégie applique l'algorithme naïf vu plus haut : \"pour chaque élément du premier dataset, je parcours le second dataset pour retrouver le ou les éléments partageant la même clé\".\n",
    "\n",
    "Ce type de jointure convient lorsque :\n",
    " * Le premier dataset est diffusable sur une jointure de type RIGHT OUTER JOIN.\n",
    " * Ou le second dataset est diffusable sur des jointures de type LEFT OUTER JOIN, LEFT SEMI JOIN ou LEFT ANTI JOIN.\n",
    " * Ou Dans tous les autres cas sur des jointures de type INNER JOIN ou équivalent.\n",
    "\n",
    "### Produit cartésien\n",
    "\n",
    "Le produit cartésien (ou CROSS JOIN) consiste à retourner toutes les combinaisons possibles de lignes sur la fusion de deux dataset.\n",
    "\n",
    "Ce type de jointure intervient lorsque :\n",
    " * Le type de jointure est INNER JOIN ou équivalent.\n",
    " * Ou `cross` est explicitement indiqué comme type de jointure\n",
    " \n",
    "Dans les autres cas, l'opération de jointure renvoie une erreur de type AnalysisException.\n",
    "\n",
    "### Exercices\n",
    "\n",
    "Pour chacun des exercices ci-dessous, vous devez :\n",
    " 1. Exécuter la cellule contenant la requête.\n",
    " 2. Observer le plan d'exécution\n",
    " 3. Retrouver et observer dans Spark UI le DAG de la requête correspondante\n",
    " \n",
    "Essayez de distinguer les différences qu'il peut y avoir entre les diverses stratégies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3586a-9088-4dfa-a5cf-6e1e7a2e6f7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Broadcast Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac5527-2e68-4fef-a5af-96ab5a85cf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.join(venues.hint(\"BROADCAST\"), checkins(\"venueId\") === venues(\"id\"))\n",
    "data.showHTML(limit=10, truncate=40)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab9438-6f4b-4d49-b56d-ecf696601459",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Shuffle Sort-Merge Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc88e1-9c65-4875-b575-6bf903f06e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.join(venues.hint(\"MERGE\"), checkins(\"venueId\") === venues(\"id\"))\n",
    "data.showHTML(limit=10, truncate=40)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadfd57-9ed8-460d-867c-1a9a20e45c36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Shuffle Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c9ac4-fdea-4742-a9aa-865f3924ce43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.join(venues.hint(\"SHUFFLE_HASH\"), checkins(\"venueId\") === venues(\"id\"))\n",
    "data.showHTML(limit=10, truncate=40)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d992f4d-ebe5-4a6c-9361-cd4b7ae423fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Shuffle-and-Replicate Nested Loop Join\n",
    "\n",
    "Pour commencer, nous allons utiliser le hint `SHUFFLE_REPLICATE_NL` pour impliquer implicitement un produit cartésien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde623d-685d-4c3a-bbcf-907eb6559e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.join(venues.hint(\"SHUFFLE_REPLICATE_NL\"), checkins(\"venueId\") === venues(\"id\"))\n",
    "data.showHTML(limit=10, truncate=40)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c802c-d0a0-4330-83b2-d059ebf70f52",
   "metadata": {},
   "source": [
    "Recherchons les lieux qui n'ont pas encore reçu de visite des utilisateurs. Pour cela, nous allons utiliser une jointure de type LEFT ANTI JOIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa00bd-a4a6-4bfe-9128-ddf6a3f3f21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = venues.join(checkins.hint(\"SHUFFLE_REPLICATE_NL\"), checkins(\"venueId\") === venues(\"id\"), \"leftanti\")\n",
    "data.showHTML(limit=10, truncate=40)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85302523-2273-4751-b2f5-e527b87e8758",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Jointure sans clé\n",
    "\n",
    "Cette fois, sans préciser la clé, nous allons générer toutes les combinaisons possibles d'utilisateurs et de lieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d51c8-6726-486b-8b74-2530e8a6e084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.select($\"userId\").join(venues)\n",
    "data.showHTML(limit=10, truncate=40)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c517ee-cc4d-462b-8f3e-ccecf852193b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
