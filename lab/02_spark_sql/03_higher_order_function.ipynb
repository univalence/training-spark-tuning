{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617242c4-f978-4e21-b194-c43b6434db97",
   "metadata": {},
   "source": [
    "# Fonction d'ordre supérieur\n",
    "\n",
    "Les fonctions d'ordre supérieur dans Spark SQL sont des fonctions qui prennent d'autres fonctions comme arguments ou qui retournent des fonctions. Ces fonctions permettent d'effectuer des opérations complexes sur des données structurées telles que des tableaux ou des structures dans Spark SQL.\n",
    "\n",
    "À partir de la version 2.4, Spark SQL a introduit plusieurs fonctions d'ordre supérieur pour travailler avec des données complexes. Voici quelques exemples de fonctions d'ordre supérieur couramment utilisées dans Spark SQL :\n",
    "\n",
    "* **transform** : Applique une fonction donnée à chaque élément d'un tableau et retourne un nouveau tableau avec les résultats.<br />Syntaxe : `transform(array, function)`.\n",
    "* **filter** : Retourne un nouveau tableau contenant les éléments qui satisfont la condition spécifiée par la fonction donnée.<br />Syntaxe : `filter(array, function)`.\n",
    "* **exists** : Vérifie si au moins un élément d'un tableau satisfait la condition spécifiée par la fonction donnée.<br />Syntaxe : `exists(array, function)`.\n",
    "* **forall** : Vérifie si tous les éléments d'un tableau satisfont la condition spécifiée par la fonction donnée.<br />Syntaxe : `forall(array, function)`.\n",
    "* **aggregate** : Agrège les éléments d'un tableau à l'aide d'une fonction d'agrégation et d'une valeur initiale.<br />Syntaxe : `aggregate(array, initial_value, merge_function[, finish_function])`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32428cf8-b4d6-4701-9c53-c11af78affe7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Préambule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a7dfa8-24a3-4426-81ab-3138506b1d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.4.1`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.4.1`\n",
    "import $ivy.`org.slf4j:slf4j-reload4j:2.0.6`\n",
    "\n",
    "import org.apache.logging.log4j.Level\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "// Avoid disturbing logs\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a591d47-3e95-415e-9177-90eb995a2e16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .progress(enable = true, keep = true, useBars = true)\n",
    "    .master(\"local[*]\")\n",
    "    // L'appel ci-dessous sert à donner un nom à votre application\n",
    "    // Ce apparaîtra notamment dans la Spark UI\n",
    "    .appName(\"SparkSQL - Fonction d'ordre supérieur\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "// Ce script fournit que élément supplémentaires pour rendre l'affichage plus confortable\n",
    "import $file.^.internal.spark_helper\n",
    "import spark_helper.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ad563-d9d0-4d2c-b99f-6ad1513bdbf2",
   "metadata": {},
   "source": [
    "## Chargement\n",
    "\n",
    "Nous allons revenir sur les commandes clients dans une cafétéria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee4f237-e002-4ae9-9220-0f064ed91c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val dataframe: DataFrame =\n",
    "  spark.read\n",
    "    // indique que le fichier contient une ligne d'en-tête qui servira\n",
    "    // pour nommer les champs\n",
    "    .option(\"header\", true)\n",
    "    // demande à Spark SQL de tenter de déterminer le type des colonnes\n",
    "    .schema(\"id STRING, client STRING, timestamp TIMESTAMP, product STRING, price DOUBLE\")\n",
    "    // lecture du fichier au format CSV\n",
    "    .csv(\"data/orders.csv\")\n",
    "\n",
    "import java.sql.Timestamp\n",
    "\n",
    "case class Order(\n",
    "  id:        String,\n",
    "  clientId:  String,\n",
    "  timestamp: Timestamp,\n",
    "  product:   String,\n",
    "  price:     Double\n",
    ")\n",
    "\n",
    "spark_helper.sparkExport(this)\n",
    "\n",
    "val orders: Dataset[Order] =\n",
    "  dataframe\n",
    "    .withColumnRenamed(\"client\", \"clientId\")\n",
    "    .as[Order]\n",
    "\n",
    "orders.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eec1306-c028-4a49-8e5c-896696601055",
   "metadata": {},
   "source": [
    "## Ensemble des prix par jour\n",
    "\n",
    "Nous voulons actuellement récupérer pour chaque jour, la liste des prix des produits vendus. Pour cela, nous allons utiliser la  fonction `collect_list(col)`.\n",
    "\n",
    "La fonction `collect_list()` est en quelque sorte l'inverse de la fonction `explode()` : au lieu de décomposer une colonne de type liste en ligne dans le dataframe, `collect_list()` ressemble des lignes du dataframe pour former une colonne de type liste.\n",
    "\n",
    "ATTENTION !!! La quantité de données collectées dans chaque liste ne devrait pas dépasser ~50 000 éléments. Au-delà de cette valeur, vous risquez de dépasser la capacité mémoire de vos exécuteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e36cc8-18f2-45ce-a862-277af699a99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val pricesByDay =\n",
    "  orders\n",
    "    .groupBy(to_date($\"timestamp\").as(\"date\"))\n",
    "    .agg(collect_list($\"price\").as(\"prices\"))\n",
    "\n",
    "pricesByDay.showHTML(limit=10, truncate=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8c8905-c117-40fb-b53c-4480d0049f40",
   "metadata": {},
   "source": [
    "## Somme des prix par jour\n",
    "\n",
    "Utilisez la fonction `aggregate(col, init, f)` pour calculer pour chaque jour le prix total de vente.\n",
    " * `col` est la colonne qui contient la liste à agréger.\n",
    " * `init` est la valeur initiale (ou la valeur à retourner si la liste dans `col` est vide). Cette valeur est de type colonne.\n",
    " * `f` est la fonction d'agrégation. Elle a pour type `(Column, Column) => Column`. Le premier paramètre de la fonction correspond au résultat intermédiaire, sachant que le premier résultat intermédiaire correspond à `init`. Le second paramètre correspond à une valeur provenant de `col`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144155ef-b8a1-4a8a-b5d2-c92532aea7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val result = ???\n",
    "\n",
    "result.showHTML(limit=10, truncate=120)\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa17500d-6993-4316-9604-2611d2dc0ec0",
   "metadata": {},
   "source": [
    "## Somme des prix hauts par jour\n",
    "\n",
    "Nous voulons calculer pour chaque jour le prix total de vente pour les articles de 2 EUR ou plus.\n",
    "\n",
    "Utilisez la fonction `filter(col, f)` pour retirer du calcul les prix de moins de 2 EUR.\n",
    " * `col` est la colonne qui contient la liste à agréger\n",
    " * `f` est la fonction de filtrage. Elle a pour type `Column => Column`. Le paramètre correspond à une valeur provenant de `col`. La fonction doit correspondre à une expression booléenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1a39e-ea73-479d-8e26-59a4de66e081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val result = ???\n",
    "\n",
    "result.showHTML(limit=10, truncate=120)\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753f8e2-5528-4ec2-8050-a6a8d201cb2e",
   "metadata": {},
   "source": [
    "Refaite l'exercice, mais en utilisant cette fois une requête SQL.\n",
    "\n",
    "Vous aurez besoin de :\n",
    " * `CAST(col AS type)` : converti le type d'une colonne.\n",
    " * `FILTER(col, f)` : filtre des valeurs dans `col` selon la fonction `f`, où `f` est une fonction de la forme `col -> condition_sur_col`.\n",
    " * `AGGREGATE(col, init, f)` : agrège les données d'une colonne selon `f`, où `f` est une fonction de la forme `(résultat, col) -> agrégation_de_résultat_et_col`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784baba-b688-4899-a33c-13d7f2e40568",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val result = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  date,\n",
    "  ??? AS total\n",
    "FROM (\n",
    "  SELECT\n",
    "    to_date(timestamp) AS date,\n",
    "    collect_list(price) AS price\n",
    "  FROM orders\n",
    "  GROUP BY date\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "result.showHTML(limit=10, truncate=120)\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a88507-dde3-40ed-87a2-75737687ece5",
   "metadata": {},
   "source": [
    "## Prix moyen par jour\n",
    "\n",
    "Pour chaque journée, nous voulons calculer le prix moyen. Malheureusement, Spark ne fournit pas de fonction qui permet de calculer une moyenne sur une liste :/\n",
    "\n",
    "Néanmoins, sans avoir à recourir à une UDF, avec l'aide de la fonction `aggregate()`, nous pouvons calculer une telle moyenne. Pour cela, nous devons :\n",
    " 1. D'un côté compter le nombre de prix et de l'autre calculer la somme des prix.\n",
    " 2. Faire la division entre le somme des prix et le décompte pour avoir la moyenne.\n",
    "\n",
    "Il y a deux façons de faire la première étape : soit utiliser 2 fois `aggregate()` pour compter et faire la somme en parallèle, soit utiliser `aggregate()` une seule fois avec une sous-structure qui stocke le décompte et la somme en même temps.\n",
    "\n",
    "À vous de voir :)\n",
    "\n",
    "Vous aurez potentiellement besoin de :\n",
    " * la fonction `struct(column_1, column_2, ...)` : permet de créer une sous structure.\n",
    " * la méthode `.cast(type)` sur une colonne : permet de convertir le type d'une colonne selon le [type passé en paramètre](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/index.html). \n",
    " * la syntaxe `<column>(\"field_name\")` sur une colonne : permet d'accéder au champ `\"field_name\"` d'une colonne, si celle-ci contient une sous-structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd766915-2ef0-4e82-b6b4-994b1a193176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val result = ???\n",
    "\n",
    "result.showHTML(limit=10, truncate=120)\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496c090-7afe-404d-8e8c-438d45a81fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
