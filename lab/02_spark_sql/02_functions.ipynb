{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98c2bbf-92eb-49c5-8641-62e09bb66fd6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Fonctions SparkSQL\n",
    "\n",
    "SparkSQL fournit un ensemble de fonctions intégrées utilisables avec les dataframes. Ces fonctions ont deux avantages :\n",
    " * Les fonctions intégrées sont généralement optimisées par le moteur Catalyst pour une meilleure performance.\n",
    " * Les fonctions intégrées sont disponibles dans toutes les installations Spark, ce qui facilite le partage et la portabilité du code.\n",
    "\n",
    "Références :\n",
    " * Scala API : https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html\n",
    " * SQL : https://spark.apache.org/docs/latest/api/sql/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec949f-d881-49dc-a8cc-4e3102fcb518",
   "metadata": {},
   "source": [
    "## Préambule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6189ab-a408-40ed-ab61-032b6b100c35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.4.1`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.4.1`\n",
    "import $ivy.`org.slf4j:slf4j-reload4j:2.0.6`\n",
    "\n",
    "import org.apache.logging.log4j.Level\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342eab3-e1f6-4e1c-b208-bb38944ff129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .progress(enable = true, keep = true, useBars = true)\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Spark SQL - Fonctions\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._\n",
    "\n",
    "import $file.^.internal.spark_helper\n",
    "import spark_helper.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f6af53-8a06-40f3-b9cd-0b47894c6f0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tweets\n",
    "\n",
    "Pour cet atelier, nous allons utiliser un dataser contenant des tweets au format JSON, suite à une recherche sur l'expression \"big data\" et datant de 2021.\n",
    "\n",
    "Note : une partie de la requête de chargement consiste à ne conserver que les lignes valides (`.where($\"_corrupt_record\".isNull)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606fd0b-9a9e-4174-8513-6e23fd89de08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "val tweets =\n",
    "  spark.read\n",
    "    .json(\"data/tweets.json.gz\")\n",
    "    .cache()\n",
    "    .where($\"_corrupt_record\".isNull)\n",
    "    .drop(\"_corrupt_record\")\n",
    "\n",
    "tweets.createOrReplaceTempView(\"tweets\")\n",
    "\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1f74c-adb1-40d1-b34d-765e2d2388d3",
   "metadata": {},
   "source": [
    "Ci-dessous, nous pouvons constater que le [schéma des tweets](https://developer.twitter.com/en/docs/twitter-api/v1/data-dictionary/object-model/tweet) est imposant et contient diverses sous-structures.\n",
    "\n",
    "Note : sur l'affichage ci-dessous, n'hésitez pas à cliquer-droit sur l'affichage du schéma et à sélectionner \"Enable Scrolling for Outputs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba527c4e-4435-4135-9561-dfcf790845a3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f9ce1-64cc-48c4-bad4-9f248d7080b1",
   "metadata": {},
   "source": [
    "## Les langues représentées\n",
    "\n",
    "Dans chaque tweet, il y a un champ `lang`, qui indique la langue dans laquelle a été écrit le tweet. La valeur de ce champ est décrite par la norme BCP47 ([liste des langues](http://www.iana.org/assignments/language-subtag-registry/language-subtag-registry)).\n",
    "\n",
    "Donner la liste des langues utilisées dans les tweets, en les classant de la plus utilisée à la moins utilisée. Faites intervenir pour cela les méthodes `.groupBy()`, `.count()` et `.orderBy()`.\n",
    "\n",
    "Pour rappel, l'accès à un champ se fait avec la notation `$\"field_name\"`. Pour classer des lignes de manière décroissante par rapport à une colonne, utilisez la méthode `.desc` sur cette colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc573389-6b6d-4ec3-adc1-3189902c0d62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "val result = ???\n",
    "\n",
    "result.explain()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3638b2-0b04-43fe-b80e-266b58fffca9",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "La fonction `count` apparaît sous deux formes dans le plan d'exécution, dans l'ordre :\n",
    " * `HashAggregate(keys=[lang#29], functions=[partial_count(1)])`\n",
    " * `HashAggregate(keys=[lang#29], functions=[count(1)])`\n",
    "\n",
    "Ce plan montre que Spark va organiser le décompte des lignes en deux phases. Dans une première phase, chaque exécuteur va compter de son côté le nombre de lignes par langue (`partial_count`). Puis, les décomptes partiels sont redistribués selon un _hash_ calculé sur la langue (`Exchange hashpartitioning(lang#3676, 200), ENSURE_REQUIREMENTS, [plan_id=2830]`), afin d'avoir les décomptes partiels d'une même langue au sein du même exécuteur. Le traitement se termine alors par la somme des décomptes partiels par langues (`count`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59036f-2e4e-4bb9-a6e0-2eb062499a3b",
   "metadata": {},
   "source": [
    "## Utilisateurs\n",
    "\n",
    "Dans un tweet, un utilisateur est représenté par une sous-structure contenant plusieurs informations, dont l'ID, le nom, le site Web...\n",
    "\n",
    "Donner la liste du _screen_name_ des utilisateurs apparaissant dans les tweets, en les classant de l'utilisateur ayant le plus tweeté à l'utilisateur ayant le moins tweeté.\n",
    "\n",
    "Note : Pour accéder à un champ `b` d'une sous-structure `a`, vous devez utiliser la syntaxe `$\"a.b\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b9ced3-efe6-4b17-be64-76a4893bed69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "val result = ???\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a10c8de-b0c9-4420-a71e-fc6acb29a622",
   "metadata": {},
   "source": [
    "## Hashtags\n",
    "\n",
    "Les tweets peuvent contenir des hashtags, qui sont des mots apparaissant dans le texte du tweet précédé d'un _hash_ (`#`).\n",
    "\n",
    "Dans la structure du tweet, les hashtags apparaissent dans le champ `entities`, qui est une structure. Cette structure contient le champ `hashtags`. Ce champ `hashtags` est un _array_ (ou liste). Celui-ci un champ `text`.\n",
    "\n",
    "Voici un extrait du schéma qui nous intéresse :\n",
    "\n",
    "```\n",
    " |-- entities: struct (nullable = true)\n",
    " |    |-- description: string (nullable = true)\n",
    " |    |-- hashtags: array (nullable = true)\n",
    " |    |    |-- element: struct (containsNull = true)\n",
    " |    |    |    |-- indices: array (nullable = true)\n",
    " |    |    |    |    |-- element: long (containsNull = true)\n",
    " |    |    |    |-- text: string (nullable = true)\n",
    "```\n",
    "\n",
    "Nous voulons la liste des hashtags les plus utilisés.\n",
    "\n",
    "Il est possible de vérifier si un tweet contient des hashtags. Pour cela, vous pouvez utiliser l'opération `.where()` et utiliser la fonction `size()` sur le champ `entities.hashtags`.\n",
    "\n",
    "Pour décomposer une liste, vous pouvez utiliser la fonction `explode()` sur le champ `$\"entities.hashtags\"` dans un `.select()`. Cette fonction crée une nouvelle pour chaque élément contenu dans la liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a004e8-4e02-47c6-a8d8-e8dfd46de63b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "val result = ???\n",
    "\n",
    "result.explain()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0cc60-9df2-417c-b2e7-1d96e874ecb3",
   "metadata": {},
   "source": [
    "## Hashtag le plus utilisé\n",
    "\n",
    "Vous allez maintenant fournir le hashtag le plus utilisé dans le dataset `tweets`.\n",
    "\n",
    "Pour récupérer la première ligne d'un dataframe, vous pouvez utiliser la méthode `.first()`. Celle-ci retourne une valeur de type `Row`. Rechercher dans la documentation SparkSQL pour comprendre comment extraire un champ de cet objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa7d06-7ffb-45b2-afc2-61ba4950bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "val hashtag: String = ???\n",
    "\n",
    "println(hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0df67f-23ee-4d34-a73a-62935cfcf18d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "tweets.rollup(\"lang\", \"user.screen_name\").count().orderBy($\"count\".desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b84615-30ab-463f-aba7-ca314b592034",
   "metadata": {},
   "source": [
    "## Ensemble des interactions entre utilisateurs\n",
    "\n",
    "Nous allons nous intéresser aux interactions visibles entre utilisateurs sur Twitter.\n",
    "\n",
    "Un tweet contient un `quoted_status` et un champ `retweeted_status`. Ces deux champs indiquent respectivement si un tweet cite un autre tweet ou si un tweet est un retweet d'un autre. Ces champs sont des sous structures avec un schéma relativement identiques. Nous trouvons notamment le champ `user`, contenant le champ `screen_name`, indiquant l'utilisateur du tweet d'origine.\n",
    "\n",
    "Il y a aussi un champ `in_reply_to_screen_name` à la racine du tweet, qui, dans le cas d'un tweet de réponse à un autre, indique l'utilisateur qui a émis le tweet initial.\n",
    "\n",
    " 1. Dans l'exercice ci-dessous, vous allez caractériser le _screen name_ de l'utilisateur à l'origine de chaque type d'interaction (citation, retweet, réponse) (`from_user`) et le _screen name_ de l'utilisateur destinataire de l'interaction (`to_user`). Par exemple, si un utilisateur A cite le tweet de l'utilisateur B, `from_user` correspond à A et `to_user` correspond à B.\n",
    " 2. Rassemblez toutes les interactions dans un seul dataframe, en utilisant l'opération `.union()`\n",
    " 3. Compter à la fois les interactions individuelles, le total des interactions émises par utilisateur, le total des interactions reçues par utilisateur et le total des interactions.\n",
    "\n",
    "Pour ce dernier point, utilisez la méthode `.cube(col_1, col_2, ...)`. Cette méthode est une généralisation de `.groupBy()`, sachant qu'elle effectue un regroupement selon toutes les combinaisons possibles entre les colonnes passées en paramètre (eg. col_1 et col_2, col_1 seule, col_2 seule, et sur l'ensemble du dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211024a8-fac9-4b64-874b-2593c665212e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val replyInteractions = ???\n",
    "\n",
    "val quotedInteractions = ???\n",
    "\n",
    "val retweetInteractions = ???\n",
    "\n",
    "val allInteractions = ???\n",
    "\n",
    "val interactionCount = ???\n",
    "\n",
    "interactionCount.explain()\n",
    "interactionCount.orderBy($\"count\".desc).showHTML(truncate=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea93af6-f022-4b1c-b5d2-09625c572525",
   "metadata": {},
   "source": [
    "## Tranches horaires les plus actives\n",
    "\n",
    "Donnez la moyenne sur l'ensemble du dataset du nombre de tweets envoyés pour chaque tranche horaire. Le champ à utiliser est le champ `created_at` exprimé en millisecondes depuis EPOCH.\n",
    "\n",
    "Dans cet exercice, vous allez avoir besoin des fonctions suivantes :\n",
    " * `to_timestamp(seconds)` : convertis un nombre (de type Long) de secondes depuis EPOCH en valeur de type Timestamp.\n",
    " * `date_trunc(section, timestamp)` : tronque un timestamp au niveau d'une section donné (`\"year\"`, `\"day\"`, `\"hour\"`...).\n",
    " * `hour(timestamp)` : retourne uniquement la section \"heure\" d'un timestamp.\n",
    " * `avg(colonne)` : retourne la moyenne sur une colonne, suite à un regroupe (eg. `.groupBy()`). Il s'agit d'une fonction d'agrégation. Elle ne peut s'utiliser que dans la méthode `.agg()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e85252-a44f-4d2f-9dd6-1266fc939484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val result = ???\n",
    "\n",
    "result.showHTML(limit=24,truncate=40)\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62474a65-eaa8-41fe-869f-768bc2777158",
   "metadata": {},
   "source": [
    "## User-Defined Function (UDF)\n",
    "\n",
    "Vous avez la possibilité de définir vos propres fonctions SparkSQL. Ces fonctions sont alors nommées des _User-Defined Function_ ou UDF.\n",
    "\n",
    "Cet exercice est divisé en deux parties. La première partie est juste dessous. Elle consiste à utiliser les fonctions SparkSQL intégrées pour calculer un timestamp et en extraire l'heure.\n",
    "\n",
    "Regarder comment sont représenter les fonction dans le plan d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97456d-fec7-4682-b6ae-91169a5f467f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "def timestamp_hour(c: Column): Column =\n",
    "  hour(to_timestamp(c))\n",
    "\n",
    "val result =\n",
    "  tweets\n",
    "    .where($\"created_at\".isNotNull)\n",
    "    .select(\n",
    "      $\"created_at\",\n",
    "      to_timestamp($\"created_at\" / 1000).as(\"timestamp\"),\n",
    "      timestamp_hour($\"created_at\" / 1000).as(\"hour\")\n",
    "    )\n",
    "\n",
    "result.explain()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d73f8ea-b298-4f8d-b087-db0fd16fe186",
   "metadata": {},
   "source": [
    "Nous allons faire la même chose ci-dessous, mais cette fois en utilisant des UDF.\n",
    "\n",
    "Pour créer une UDF, vous devez utiliser la fonction `udf[Out, In](f).withName(\"<udf_name>\")`, où `Out` est le type de sortie, `In` est le type d'entrée de la fonction, `f` est la fonction qui sera utilisée par l'UDF et `<udf_name>` est le nom de la fonction qui apparaîtra dans le plan d'exécution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c996a0ba-c1ae-4f80-b051-8aeaabc29b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data\n",
    "\n",
    "import java.time._\n",
    "import java.sql.Timestamp\n",
    "\n",
    "def toTimestamp(epochMilli: Long): Timestamp = {\n",
    "  new Timestamp(epochMilli)\n",
    "}\n",
    "\n",
    "def hourOfEpochMilli(epochMilli: Long): Int = {\n",
    "  val dateTime = toTimestamp(epochMilli).toLocalDateTime()\n",
    "  dateTime.getHour()\n",
    "}\n",
    "\n",
    "val toTimestamp_udf = ???\n",
    "val hourOfEpochMilli_udf = ???\n",
    "\n",
    "val result =\n",
    "  tweets\n",
    "    .where($\"created_at\".isNotNull)\n",
    "    .select(\n",
    "      $\"created_at\",\n",
    "      toTimestamp_udf($\"created_at\").as(\"timestamp\"),\n",
    "      hourOfEpochMilli_udf($\"created_at\").as(\"hour\")\n",
    "    )\n",
    "\n",
    "result.explain()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07934b8e-c125-4014-ac1d-eb6245e8ecfc",
   "metadata": {},
   "source": [
    "Vous allez maintenant faire la même chose, mais cette fois en utilisant le langage SQL.\n",
    "\n",
    "Pour pouvoir utiliser une UDF dans une requête SQL, il d'abord enregistrer l'UDF. Pour cela, il faut utiliser la méthode `spark.udf.register(\"<nom_SQL>\", udf)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f099436-46e3-4ff7-b955-9ce15dc737e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "???\n",
    "\n",
    "val result =\n",
    "  spark.sql(\"\"\"\n",
    "SELECT created_at, toTimestamp(created_at) AS timestamp, hourOfEpochMilli(created_at) AS hour\n",
    "FROM tweets\n",
    "\"\"\")\n",
    "\n",
    "result.showHTML(limit=10, truncate=40)\n",
    "result.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935aee7a-d235-44f5-9b6a-f05b217f6465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
