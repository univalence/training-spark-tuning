{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68479a4-719b-444e-89f5-a3743865c26d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Jointure\n",
    "\n",
    "D'une mani√®re g√©n√©rale, le sujet des jointures en big data est un des sujets de ce domaine les plus complexes, mais aussi sur lequel il y a le plus d'enjeu, tant nous avons une vision relationnelle de la donn√©e.\n",
    "\n",
    "Le principe d'une jointure consiste, en effet, √† fusionner deux datasets en recherchant dans les deux datasets les lignes qui partagent la m√™me cl√©. Un algorithme na√Øf va r√©soudre une jointure en $O(n^2)$ :\n",
    "\n",
    "> pour chaque √©l√©ment du premier dataset, je parcours le second dataset pour retrouver le ou les √©l√©ments paratageant la m√™me cl√©.\n",
    "\n",
    "Dans un contexte big data, une telle complexit√© n'est pas envisageable, du fait de la taille des donn√©es √† traiter, des communications r√©seau que cela peut engendrer et de la capacit√© de stockage en m√©moire vive qui reste relativement petite.\n",
    "\n",
    "Il existe heureusement des algorithmes de jointure, propos√©s par Spark, bien plus efficaces, qui vont consister √† utiliser une indexation sur la cl√© et/ou un tri sur la cl√© et/ou la diffusion d'un dataset entier sur les diff√©rents executors pour r√©aliser les op√©rations de recherche en local.\n",
    "\n",
    "Spark, par d√©faut, va rechercher l'algorithme le plus adapt√© et le plus performant, en fonction du type de jointure et de statistiques obtenues sur les donn√©es. Mais, vous avez la possibilit√© de forcer Spark √† adopter un algorithme.\n",
    "\n",
    "**Dans ce notebook**, vous allez voir les diff√©rents algorithmes propos√©s par Spark et leurs cons√©quences, ainsi que les diff√©rents types de jointures possibles et ce qu'elles impliquent en termes de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d4b41-1a01-47bf-9ac2-6db0fb40a8d2",
   "metadata": {},
   "source": [
    "## Pr√©lude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f4e136-2975-47dc-8c84-677735b45c09",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.slf4j:slf4j-reload4j:2.0.6`\n",
    "import $ivy.`org.apache.logging.log4j:log4j-api:2.8.2`\n",
    "import $ivy.`org.apache.logging.log4j:log4j-slf4j-impl:2.8.2`\n",
    "\n",
    "// Avoid disturbing logs\n",
    "import org.apache.log4j._\n",
    "import org.apache.log4j.varia._\n",
    "BasicConfigurator.configure(NullAppender.getNullAppender())\n",
    "\n",
    "import $ivy.`org.apache.spark::spark-core:3.2.1`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.2.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c6ea18-3a5f-4877-932a-c33aacd24b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Spark tuning - Jointure\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a3be6-2f88-413b-b6f5-8da8755c8e3f",
   "metadata": {},
   "source": [
    "## Chargement des datasets\n",
    "\n",
    "Nous allons utiliser dans ce notebook des datasets diff√©rents de ce qui a √©t√© vu jusque-l√†. Nous nous mettons dans le cadre d'une application sur t√©l√©phone, qui permet √† ses utilisateurs de partager sa position avec leurs amis. Pour cela, l'application d√©tecte par g√©olocalisation le lieu et propose √† l'utilisateur de partager le lieu (_checkin_). Une gamification est ajout√©e, qui permet √† l'utilisateur de gagner des badges selon la fr√©quence de ses checkins, la distance entre deux chekins, l'utilisateur qui effectue r√©guli√®rement les premiers checkins de la journ√©e d'un m√™me lieu devient \"Maire\" de ce lieu...\n",
    "\n",
    "Nous avons deux datasets :\n",
    "\n",
    "* Venues : repr√©sente un ensemble de lieux enregistr√©s qu'il est possible de visiter. Il contient notamment l'identifiant du lieu et ses coordonn√©es.\n",
    "* Checkins : repr√©sente l'ensemble des checkins r√©alis√©s par les utilisateurs de l'application. Il contient l'identifiant de l'utilisateur, l'identifiant du lieu et le timestamp du checkin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10cc1fd-e291-4ee1-9b1e-326afd9a7cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import java.time.Instant\n",
    "\n",
    "case class Venue(id: String, latitude: Double, longitude: Double, locationType: String, country: String)\n",
    "case class Checkin(userId: String, venueId: String, timestamp: Instant)\n",
    "\n",
    "val venuesFilename   = \"data/threetriangle/venues.txt.gz\"\n",
    "val checkinsFilename = \"data/threetriangle/checkins.txt.gz\"\n",
    "\n",
    "val venues =\n",
    "  spark.read\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    .schema(\"id STRING, latitude DOUBLE, longitude DOUBLE, locationType STRING, country STRING\")\n",
    "    .csv(venuesFilename)\n",
    "    .as[Venue]\n",
    "\n",
    "val checkins =\n",
    "  spark.read\n",
    "    .option(\"sep\", \"\\t\")\n",
    "    // Option to correctly interpret timestamp in checkin data\n",
    "    .option(\"timestampFormat\", \"EEE MMM d HH:mm:ss Z yyyy\")\n",
    "    .schema(\"userId STRING, venueId STRING, timestamp TIMESTAMP, tzOffset INT\")\n",
    "    .csv(checkinsFilename)\n",
    "    .as[Checkin]\n",
    "\n",
    "venues.createOrReplaceTempView(\"venues\")\n",
    "checkins.createOrReplaceTempView(\"checkins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb241a-30ba-4c69-a4f1-c9bce997a7c6",
   "metadata": {},
   "source": [
    "## Jointure\n",
    "\n",
    "Nous voulons maintenant retrouver les informations des diff√©rents lieux o√π ont √©t√© effectu√©s les checkins. Nous avons donc besoin de r√©aliser une jointure entre les datasets Checkins et Venues.\n",
    "\n",
    "La m√©thode `.join()` est assez simple √† utiliser. Elle s'applique sur un premier dataframe, puis vous sp√©cifiez en param√®tre le dataframe avec lequel vous cr√©ez une jointure, en indiquant √©ventuellement la relation de jointure et le type de jointure (inner, outer, left outer, right outer..., par d√©faut : inner)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda33502-59e6-46d8-bd9f-f73b4ed9e358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.join(venues, checkins(\"venueId\") === venues(\"id\"))\n",
    "data.show(numRows = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b8855-c80f-42b9-a9b3-00844eec0d6b",
   "metadata": {},
   "source": [
    "Affichons le plan d'ex√©cution de cette requ√™te. Essayer de percevoir les diff√©rentes, dont celles qui concernent la jointure (Quelles optimisations sont apport√©es ? Y a-t-il des √©changes de donn√©es ?)\n",
    "\n",
    "Note : un plan d'ex√©cution se lit du bas vers le haut pour suivre les diff√©rentes √©tapes de traitement dans l'ordre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e386a-29dd-4148-ba61-c0dc60e2e8ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517903ff-1464-43ce-ab8f-3c859f167fa2",
   "metadata": {},
   "source": [
    "Allez dans Spark UI et explorez l'onglet \"SQL / DataFrame\". Spark UI montre un DAG assez complexe pour la requ√™te.\n",
    "\n",
    "**Approche alternative**\n",
    "\n",
    "Voici l'√©quivalent de notre requ√™te, mais cette fois exprim√©e en SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc943e7b-25fb-4419-bb07-0a9553a85f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data =\n",
    "  spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  checkins c INNER JOIN venues v ON (v.id = c.venueId)\n",
    "\"\"\")\n",
    "\n",
    "data.show(numRows = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d533d-e406-4377-b301-36a6548fae32",
   "metadata": {},
   "source": [
    "ü§î **Question** ü§î\n",
    "\n",
    "Le plan d'ex√©cution de cette requ√™te est-il similaire au plan pr√©c√©dent ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9860e9b-4c23-40ac-849a-758b3de68551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75647c-d147-4cb2-b008-3cd21781f2ac",
   "metadata": {},
   "source": [
    "üëÄ **Ce qu'il faut voir** üëÄ\n",
    "\n",
    "Dans le processus de g√©n√©ration du plan d'ex√©cution physique, nous pouvons voir que Spark fini par s√©lectionner une strat√©gie de jointure. Cette strat√©gie se traduit par une phase d'√©change (ou plus exactement, de diffusion) de donn√©es entre ex√©cuteurs (`BroadcastExchange`). Puis, interviens la phase de jointure bas√©e sur le _hash_ de la cl√© de jointure (`BroadcastHashJoin`).\n",
    "\n",
    "Nous pouvons voir aussi que Spark se laisse la possibilit√© au dernier moment de changer de strat√©gie de jointure (`AdaptiveSparkPlan isFinalPlan=false`), sur la base de donn√©es statistiques r√©cup√©r√©es pendant l'ex√©cution.\n",
    "\n",
    "Ce type de jointure est similaire √† ce que nous avons vu avec les variables broadcast dans le cas des RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637c689-4fb5-4bb3-a548-a57fa1524920",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Strat√©gie de jointure\n",
    "\n",
    "Spark SQL dispose de diff√©rentes strat√©gies de jointure. Nous venons d'en voir une dans la section pr√©c√©dente.\n",
    "\n",
    "Sans pr√©cision dans le code, cette strat√©gie est choisie selon une heuristique param√©trable li√©e, notamment, au type de la jointure, la condition sur les cl√©s (√©quivalence ou non-√©quivalence), √† taille des donn√©es ou √† d'autres donn√©es statistiques.\n",
    "\n",
    "Si vous voulez forcer la strat√©gie de jointure, vous pouvez le pr√©ciser dans le code √† travers des _hints_ :\n",
    "\n",
    "```scala\n",
    "  df1.join(df2.hint(\"<Strat√©gie>\"), ...)\n",
    "```\n",
    "\n",
    "En SQL :\n",
    "\n",
    "```scala\n",
    "  spark.sql(\"\"\"SELECT /*+ <Strat√©gie> */ ...\"\"\")\n",
    "```\n",
    "\n",
    "Voici les diff√©rentes strat√©gies (et les valeurs √† utiliser dans les hints) :\n",
    "\n",
    " * Broadcast Hash Join (BROADCAST / BROADCASTJOIN / MAPJOIN)\n",
    " * Shuffle Sort-Merge Join (MERGE / SHUFFLE_MERGE / MERGEJOIN)\n",
    " * Shuffle Hash Join (SHUFFLE_HASH)\n",
    " * Shuffle-and-Replicate Nested Loop Join (SHUFFLE_REPLICATE_NL)\n",
    "\n",
    "### Broadcast Hash Join\n",
    "\n",
    "Cette strat√©gie est utilis√©e dans ces conditions :\n",
    " * La relation de jointure se base sur l'√©galit√© entre les cl√©s (ie. `dataset1(\"key\") === dataset2(\"key\")`).\n",
    " * L'un des datasets est consid√©r√© comme √©tant suffisamment petit.\n",
    " * Tous les types de jointure sont support√©s, sauf FULL OUTER JOIN.\n",
    "\n",
    "Dans ce cas, le \"petit\" dataset est transmis (ou diffus√©, d'o√π le terme _broadcast_) sous la forme d'une table de hachage (_hash table_) √† l'ensemble des ex√©cuteurs participant √† la jointure. La jointure est alors r√©alis√©e en local sur chaque n≈ìud.\n",
    "\n",
    "Le seuil indiquant si la strat√©gie sera utilis√©e est fix√©e par le param√®tre `spark.sql.autoBroadcastJoinThreshold`. Il est exprim√© en octets (valeur par d√©faut : 10485760 (= 10 MB)). Si l'un des datasets de la jointure √† une taille inf√©rieure √† ce seuil, il sera diffus√©. Si aucun \n",
    "\n",
    "Dans le cadre de la fonctionnalit√© AQE (_Adaptive Query Execution_), qui permet de pousser Spark √† s√©lectionner une autre start√©gie sur la base de relev√©s statistiques, au lieu de la strat√©gie planifi√©e initialement, le seuil permettant de passer √† la strat√©gie _Broadcast Hash Join_ est fix√© par le param√®tre `spark.sql.adaptive.autoBroadcastJoinThreshold` exprim√© en octet. Si vous fixez sa valeur √† -1, cette fonctionnalit√© est d√©sactiv√©e.\n",
    "\n",
    "Un autre param√®tre intervient : `spark.sql.broadcastTimeout`. Ce param√®tre est exprim√© en secondes (valeur par d√©faut¬†: 300 (= 5mn)). Il est d√©tect√© des probl√®mes de communication durant la diffusion des donn√©es. Si la diffusion de la table ne peut pas √™tre termin√©e dans le d√©lai imparti, Spark interrompt l'ex√©cution de la requ√™te et g√©n√®re une erreur.\n",
    "\n",
    "### Shuffle Sort-Merge Join\n",
    "\n",
    "Cette strat√©gie est utilis√©e dans ces conditions :\n",
    " * La relation de jointure se base sur l'√©galit√© entre les cl√©s (ie. `dataset1(\"key\") === dataset2(\"key\")`).\n",
    " * Il est possible de trier les cl√©s.\n",
    " * Tous les types de jointure sont support√©s.\n",
    " \n",
    "C'est la strat√©gie utilis√©e par d√©faut (sauf dans le cas ou le param√®tre `spark.sql.join.preferSortMergeJoin=false`, dans ce cas, Spark utilisera _Shuffle Hash Join_).\n",
    "\n",
    "Les √©tapes de jointure sont :\n",
    " 1. √âchanges des donn√©es entre ex√©cuteurs en fonction de la cl√©.\n",
    " 2. Tri des donn√©es en local en fonction de la cl√©.\n",
    " 3. Fusion des deux datasets.\n",
    "\n",
    "En g√©n√©ral, _Shuffle Sort-Merge Join_ a de meilleures performances sur des donn√©es volumineuses.\n",
    "\n",
    "### Shuffle Hash Join\n",
    "\n",
    "Cette strat√©gie est utilis√©e dans ces conditions :\n",
    " * La relation de jointure se base sur l'√©galit√© entre les cl√©s (ie. `dataset1(\"key\") === dataset2(\"key\")`).\n",
    " * Il est possible de trier les cl√©s.\n",
    " * Tous les types de jointure sont support√©s, sauf FULL OUTER JOIN.\n",
    "\n",
    "Les √©tapes de jointure sont :\n",
    " 1. √âchanges des donn√©es entre ex√©cuteurs en fonction de la cl√©.\n",
    " 2. D√©p√¥t des donn√©es transf√©r√©es en local dans une table de hachage en fonction de la cl√©.\n",
    " 3. Fusion des deux datasets.\n",
    "\n",
    "_Shuffle Hash Join_ est efficace lorsque la cl√© utilis√©e pour la jointure permet un partitionnement √©quilibr√© de la donn√©e.\n",
    "\n",
    "### Shuffle-and-Replicate Nested Loop Join\n",
    "\n",
    "Cette strat√©gie applique l'algorithme na√Øf vu plus haut : \"pour chaque √©l√©ment du premier dataset, je parcours le second dataset pour retrouver le ou les √©l√©ments partageant la m√™me cl√©\".\n",
    "\n",
    "Ce type de jointure convient lorsque :\n",
    " * Le premier dataset est diffusable sur une jointure de type RIGHT OUTER JOIN.\n",
    " * Ou le second dataset est diffusable sur des jointures de type LEFT OUTER JOIN, LEFT SEMI JOIN ou LEFT ANTI JOIN.\n",
    " * Ou Dans tous les autres cas sur des jointures de type INNER JOIN ou √©quivalent.\n",
    "\n",
    "### Produit cart√©sien\n",
    "\n",
    "Le produit cart√©sien (ou CROSS JOIN) consiste √† retourner toutes les combinaisons possibles de lignes sur la fusion de deux dataset.\n",
    "\n",
    "Ce type de jointure intervient lorsque :\n",
    " * Le type de jointure est INNER JOIN ou √©quivalent.\n",
    " * Ou `cross` est explicitement indiqu√© comme type de jointure\n",
    " \n",
    "Dans les autres cas, l'op√©ration de jointure renvoie une erreur de type AnalysisException.\n",
    "\n",
    "### Exercices\n",
    "\n",
    "üë∑ Pour chacun des exercices ci-dessous, vous devez :\n",
    " 1. Ex√©cuter la cellule contenant la requ√™te.\n",
    " 2. Observer le plan d'ex√©cution\n",
    " 3. Retrouver et observer dans Spark UI le DAG de la requ√™te correspondante\n",
    " \n",
    "Essayez de distinguer les diff√©rences qu'il peut y avoir entre les diverses strat√©gies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3586a-9088-4dfa-a5cf-6e1e7a2e6f7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Broadcast Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac5527-2e68-4fef-a5af-96ab5a85cf3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.join(venues.hint(\"BROADCAST\"), checkins(\"venueId\") === venues(\"id\"))\n",
    "data.show(numRows = 10)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab9438-6f4b-4d49-b56d-ecf696601459",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Shuffle Sort-Merge Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc88e1-9c65-4875-b575-6bf903f06e94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.join(venues.hint(\"MERGE\"), checkins(\"venueId\") === venues(\"id\"))\n",
    "data.show(numRows = 10)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadfd57-9ed8-460d-867c-1a9a20e45c36",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Shuffle Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c9ac4-fdea-4742-a9aa-865f3924ce43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.join(venues.hint(\"SHUFFLE_HASH\"), checkins(\"venueId\") === venues(\"id\"))\n",
    "data.show(numRows = 10)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af173d-4954-4fdc-af45-33b327920c98",
   "metadata": {},
   "source": [
    "üëÄ **Question** üëÄ\n",
    "\n",
    "En comparant dans Spark UI avec la pr√©c√©dente strat√©gie, quel pourrait √™tre la meilleure strat√©gie pour nos dataframe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d992f4d-ebe5-4a6c-9361-cd4b7ae423fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Shuffle-and-Replicate Nested Loop Join\n",
    "\n",
    "Pour commencer, nous allons utiliser le hint `SHUFFLE_REPLICATE_NL` pour impliquer implicitement un produit cart√©sien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dde623d-685d-4c3a-bbcf-907eb6559e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.join(venues.hint(\"SHUFFLE_REPLICATE_NL\"), checkins(\"venueId\") === venues(\"id\"))\n",
    "data.show(numRows = 10)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c802c-d0a0-4330-83b2-d059ebf70f52",
   "metadata": {},
   "source": [
    "Recherchons les lieux qui n'ont pas encore re√ßu de visite des utilisateurs. Pour cela, nous allons utiliser une jointure de type LEFT ANTI JOIN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa00bd-a4a6-4bfe-9128-ddf6a3f3f21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = venues.join(checkins.hint(\"SHUFFLE_REPLICATE_NL\"), checkins(\"venueId\") === venues(\"id\"), \"leftanti\")\n",
    "data.show(numRows = 10)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85302523-2273-4751-b2f5-e527b87e8758",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Jointure sans cl√©\n",
    "\n",
    "Cette fois, sans pr√©ciser la cl√©, nous allons g√©n√©rer toutes les combinaisons possibles d'utilisateurs et de lieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399d51c8-6726-486b-8b74-2530e8a6e084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = checkins.select($\"userId\").join(venues)\n",
    "data.show(numRows = 10)\n",
    "data.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b113597-2ad9-492b-ad75-55e02e370a15",
   "metadata": {},
   "source": [
    "## Influence du type de jointure sur la strat√©gie choisie\n",
    "\n",
    "Nous allons voir, comment le type de jointure influe sur la strat√©gie de jointure choisie au sein de Spark. Pour rappel, les notions de dataframe gauche et de dataframe droite sont per√ßues par rapport √† l'op√©ration :\n",
    "\n",
    "`<left_dataframe>.join(<right_dataframe>, ...)`\n",
    "\n",
    "Voici les diff√©rents types de jointure :\n",
    "\n",
    "* **inner** : par d√©faut, conserve les √©l√©ments ayant une correspondance de chaque c√¥t√© de la jointure.\n",
    "* **outer, full, fullouter, full_outer** : tente de trouver une correspondance √† tous les √©l√©ments des deux dataframes, ou en associant des valeurs nulles par d√©faut.\n",
    "* **leftouter, left, left_outer** : tente de trouver une correspondance pour tous les √©l√©ments de dataframe de gauche, des valeurs nulles sont utilis√©es par d√©faut, pour les √©l√©ments qui ne poss√®dent pas de correspondant √† droite.\n",
    "* **rightouter, right, right_outer** : tente de trouver une correspondance pour tous les √©l√©ments de dataframe de droite, des valeurs nulles sont utilis√©es par d√©faut, pour les √©l√©ments qui ne poss√®dent pas de correspondant √† gauche.\n",
    "* **leftsemi, left_semi, semi** : √©quivalent √† un _inner join_, sauf que les colonnes du dataframe de droite sont retir√©es en sortie. Ce type de jointure permet de v√©rifier que des valeurs d'un dataframe apparaissent bien dans un autre dataframe.\n",
    "* **leftanti, left_anti, anti** : recherche dans le dataframe de gauche les cl√©s qui n'apparaissent pas dans le dataframe de droite. C'est l'oppos√© de _left semi join_. \n",
    "* **cross** : produit cart√©sien, essaye toutes les combinaisons de correspondances entre les √©l√©ments gauches et droites.\n",
    "\n",
    "Ici, nous allons utiliser des donn√©es plus simples : un ensemble d'utilisateurs, que nous chargeons directement en m√©moire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26b210-2fd2-4d36-82cb-5a152344d6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val users =\n",
    "  Seq(\n",
    "    (\"123\", \"1\", 32),\n",
    "    (\"456\", \"2\", 25),\n",
    "    (\"789\", \"3\", 16),\n",
    "    (\"321\", \"8\", 55)\n",
    "  ).toDF(\"id\", \"name_id\", \"age\")\n",
    "\n",
    "val mapping =\n",
    "  Seq(\n",
    "    (\"1\", \"Jon\"),\n",
    "    (\"2\", \"Mary\"),\n",
    "    (\"3\", \"Tom\"),\n",
    "    (\"4\", \"Albert\")\n",
    "  ).toDF(\"id\", \"name\")\n",
    "\n",
    "val adults = users.where($\"age\" >= 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f5ca8f-364e-40c9-99a3-c71948042f04",
   "metadata": {},
   "source": [
    "ü§î **Question** ü§î\n",
    "\n",
    "* Pour chaque cellule ci-dessous, quelles principales diff√©rences relevez-vous concernant le plan d'ex√©cution ?\n",
    "* Regroupez les diff√©rents types de jointure selon l'√©quivalence des plans d'ex√©cution. Expliquez pourquoi certains plans sont similaires pour des types de jointure diff√©rents ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76288028-c192-4481-ac05-3e91f0a6af20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = adults.join(mapping, adults(\"name_id\") === mapping(\"id\"), \"inner\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26d708f-6d13-454e-a70a-37ef8143d19d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = adults.join(mapping, adults(\"name_id\") === mapping(\"id\"), \"outer\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8b212a-3eac-416e-9247-60c0b5697fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = adults.join(mapping, adults(\"name_id\") === mapping(\"id\"), \"left_outer\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d0483-093a-404b-a203-75822e6c2ed7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = adults.join(mapping, adults(\"name_id\") === mapping(\"id\"), \"right_outer\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0171f7-18b9-4d0f-9d92-89060343310e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = adults.join(mapping, adults(\"name_id\") === mapping(\"id\"), \"left_semi\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6f05c9-741a-4fc2-b185-b0fc12077483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data = adults.join(mapping, adults(\"name_id\") === mapping(\"id\"), \"left_anti\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e5946-d224-4c40-ac89-c52471731193",
   "metadata": {},
   "source": [
    "## Auto-jointure (_self join_)\n",
    "\n",
    "L'auto-jointure est une jointure o√π le dataframe de gauche et de droite sont les m√™mes.\n",
    "\n",
    "Nous avons la table d'employ√©s ci-dessous, qui indique dans la colonne `superior` l'employ√© responsable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6be52-40f8-4204-92d9-5501bd6f5246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val employees =\n",
    "  Seq(\n",
    "    (\"123\", \"Jon\", null),\n",
    "    (\"456\", \"Mary\", \"123\"),\n",
    "    (\"789\", \"Tom\", \"123\"),\n",
    "    (\"321\", \"Frank\", null)\n",
    "  ).toDF(\"id\", \"name\", \"superior\")\n",
    "\n",
    "employees.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd9a584-b21d-42b9-a51c-af6d756bb6cf",
   "metadata": {},
   "source": [
    "Nous voulons afficher chaque employ√© son responsable, s'il en a un."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79813b4-2aa4-4fc0-b89d-e05c255638b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val data =\n",
    "  employees.as(\"l\")\n",
    "    .join(employees.as(\"r\"), $\"l.id\" === $\"r.superior\")\n",
    "    .select($\"l.id\", $\"r.name\", $\"l.name\" as \"superior\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff250df2-d1f2-48eb-868d-8298a68ee668",
   "metadata": {},
   "source": [
    "ü§î **Question** ü§î\n",
    "\n",
    "* Quelle strat√©gie est appliqu√©e ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdca9e-5e57-4e6d-a2f6-3fcbbf3bbff2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.17"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
