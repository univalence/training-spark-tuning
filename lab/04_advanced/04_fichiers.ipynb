{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "639a668c-285b-4fc6-97ae-f6bbe1fe88bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Manipulation de fichier\n",
    "\n",
    "SparkSQL est capable de lire et √©crire des donn√©es depuis ou vers des fichiers de diff√©rents formats. Ces fichiers peuvent √™tre des fichiers simples, des fichiers compress√©s, des fichiers partitionn√©s, des fichiers partitionn√©s sur HDFS.\n",
    "\n",
    "Dans ce notebook, nous allons voir comment se comporte SparkSQL avec les fichiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d2ac66-e228-4e0d-8afd-94ed51cb292e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pr√©ambule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e849459-ac53-4b30-9e48-fe2463873d8b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.slf4j:slf4j-reload4j:2.0.6`\n",
    "import $ivy.`org.apache.logging.log4j:log4j-api:2.8.2`\n",
    "import $ivy.`org.apache.logging.log4j:log4j-slf4j-impl:2.8.2`\n",
    "\n",
    "// Avoid disturbing logs\n",
    "import org.apache.log4j._\n",
    "import org.apache.log4j.varia._\n",
    "BasicConfigurator.configure(NullAppender.getNullAppender())\n",
    "\n",
    "import $ivy.`org.apache.spark::spark-core:3.2.1`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.2.1`\n",
    "import $ivy.`org.apache.spark::spark-hive:3.2.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730ca2d7-2034-448f-bc66-8dc87928e8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .appName(\"Spark tunning - Fichiers\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    .config(\"spark.sql.warehouse.dir\", \"target/spark-warehouse\")\n",
    "    // .config(\"spark.hadoop.hive.metastore.warehouse.dir\", \"target/metastore\")\n",
    "    // .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bac0a3-e994-4ad8-b6a7-ce6a020e0b03",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ex√©cutez la cellule ci-dessous √† chaque fois que vous souhaitez recommencer les exercices plus bas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bca0b96-cc94-457e-97dd-efe0fc28a07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanTarget = {\n",
    "    import java.nio.file.{Files, Path, Paths}\n",
    "\n",
    "    def deleteDirectory(path: Path): Unit = {\n",
    "      if (Files.isDirectory(path)) {\n",
    "        // List the directory contents and delete them recursively\n",
    "        Files.list(path).forEach(deleteDirectory)\n",
    "      }\n",
    "      // Delete the file or directory (if it's a directory, it should be empty by now)\n",
    "      try { Files.delete(path) } catch { case _: java.nio.file.NoSuchFileException => () }\n",
    "    }\n",
    "\n",
    "    val targetDirectory = Paths.get(\"target/\")\n",
    "    deleteDirectory(targetDirectory)\n",
    "    Files.createDirectory(targetDirectory)\n",
    "}\n",
    "\n",
    "cleanTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e3d8b-01b7-46a3-8b4a-9597dcef71eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lecture d'un fichier JSON compress√©\n",
    "\n",
    "SparkSQL est capable de g√©rer naturellement les fichiers compress√©s, sur diff√©rents algorithmes de compression (gzip, bzip2, snappy, lz4, zstd...). La compression permet de gagner de l'espace de stockage et d'augmenter le d√©bit du transfert de donn√©es. Il sera en g√©n√©ral plus efficace sur les fichiers textes que sur les fichiers binaires. La compression demande un peu plus d'utilisation CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c546bb-b147-45b1-b719-c597f291718c",
   "metadata": {},
   "source": [
    "Utilisez Spark pour charger le fichier JSON compress√©.\n",
    "\n",
    "La m√©thode `.repartition()` plus bas permet de forcer la redistribution des donn√©es dans plusieurs partitions. La valeur pass√©e en param√®tre correspond au nombre de partitions souhait√©. Cette valeur est limit√©e par le nombre de Core/CPU disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a4563-2df2-4edf-85a5-aee6a0cb1ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val rawDataframe =\n",
    "  spark.read\n",
    "    .json(\"data/tweets.json.gz\")\n",
    "    .cache()\n",
    "    .where($\"_corrupt_record\" isNull)\n",
    "    .drop(\"_corrupt_record\")\n",
    "\n",
    "val dataframe =\n",
    "  rawDataframe\n",
    "    .repartition(4)\n",
    "\n",
    "dataframe.show(numRows = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e2ec5-024c-4c9c-b89f-bd60da57c8b7",
   "metadata": {},
   "source": [
    "Nous allons voir combien de partitions sont associ√©es au dataframe. Pour cela, nous allons utiliser l'interface RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c0153-7bb8-4493-b4bd-21c9bde91898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c78c21-2c98-4cea-bf4c-c8913800391f",
   "metadata": {},
   "source": [
    "## Sauvegarde dans des fichiers Parquet\n",
    "\n",
    "Nous allons maintenant tester diff√©rents algorithmes de compression.\n",
    "\n",
    "La fonction ci-dessous va permettre de visualiser pour chaque algorithme ses performances et termes de capacit√© de compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff4e06-cdfa-46ec-abc6-88e93227f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSaveParquet(dataframe: DataFrame, alg: String): Unit = {\n",
    "  val file = s\"tweets-$alg.parquet\"\n",
    "  dataframe.repartition(8).write.option(\"compression\", alg).parquet(s\"target/$file\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d060ee6-03a1-47f3-8ee2-125e78cfc563",
   "metadata": {
    "tags": []
   },
   "source": [
    "üë∑ Dans chaque cas ci-dessous, regardez et comparez les diff√©rents r√©sultats obtenus, aussi bien dans Spark UI qu'au niveau du syst√®me de fichiers (√† gauche de l'√©cran)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907de5b5-7a91-4fb6-ba22-1bfad90bd859",
   "metadata": {},
   "source": [
    "### Pas de compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2db99-d7cd-47dc-a338-ea8e9f9bf6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testSaveParquet(dataframe, \"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9bb81-712b-4e3c-a3bd-0c195fb9985d",
   "metadata": {},
   "source": [
    "### Snappy compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aef6bf-01c6-4980-a579-7f7ff53bc70a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testSaveParquet(dataframe, \"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31c241-d722-4e32-8fb9-49b52cf2bcf2",
   "metadata": {},
   "source": [
    "### GZip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d10d8d-60e3-47c3-bcdd-e18c39ef4e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testSaveParquet(dataframe, \"gzip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328d480-8e7a-4550-9009-218f28dc4fb8",
   "metadata": {},
   "source": [
    "### ZStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64317a10-c7c2-4ab3-9491-e5c5bec8eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "testSaveParquet(dataframe, \"zstd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644cdfd-0edf-4ff2-9a3b-4b2d7ad90738",
   "metadata": {},
   "outputs": [],
   "source": [
    "val tweets = spark.read.parquet(\"target/tweets-gzip.parquet\")\n",
    "\n",
    "tweets.show(numRows = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310426ec-1947-4835-ba39-4a382b1d5436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d4bc66-7576-403f-81f4-93e033988385",
   "metadata": {},
   "source": [
    "## Gestion par Spark\n",
    "\n",
    "En utilisant les m√©thodes `.saveAsTable()` et `table()`, Spark va g√©rer sont propre espace de stockage. Ici, il s'agit de `target/spark-warehouse` (par d√©faut, `spark-warehouse`).\n",
    "\n",
    "üë∑ Ex√©cuter les cellules ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bcefdf-9348-4a13-b54d-0cd74f855d5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe.write.mode(\"overwrite\").saveAsTable(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a68c1-e105-4ccb-b6d5-2df234655230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read.table(\"tweets\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41890534-6598-484f-a3b4-b918e1b56111",
   "metadata": {},
   "source": [
    "ü§î **Question** ü§î\n",
    "\n",
    "* Explorez le r√©pertoire `target/spark-warehouse`. Comment est-ce que Spark organise le stockage et quel fomrat est utilis√© ?\n",
    "* Quelles diff√©rences apparaissent avec l'approche vu pr√©c√©demment ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64d26c0-7e73-4b89-91b7-df2b55fc324b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
