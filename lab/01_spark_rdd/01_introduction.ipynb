{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377b833b-f8a1-444f-8e60-83c86ec5de3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Introduction à Spark Core\n",
    "\n",
    "Dans ce notebook, nous allons découvrir pas à pas l'API Spark Core et les RDD (pour _Resilient Distributed Dataset_).\n",
    "\n",
    "N'hésitez pas à aller voir la documentation de Spark.\n",
    " * Scaladoc de l'API Spark : https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e84e24-73c4-4bab-8f42-f0df543d1c8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import des dépendances Spark\n",
    "Le noyau [Almond](https://almond.sh/) va nous permettre de récupérer les dépendances Spark.\n",
    "\n",
    "⚠️ Elles n'en ont pas l'air, mais les deux lignes ci-dessous peuvent potentiellement importer un grand nombre de dépendances. À chaque fois qu'une dépendance est en cours d'import ou est importée, une ligne est ajoutée dans la partie output (en fond rose). Pour éviter d'avoir trop de lignes affichées, cliquez droit sur la partie output et sélection \"Enable Scrolling for Outputs\". L'import des dépendances est terminé lorsqu'un numéro apparaît entre crochets, à la place d'une étoile, à gauche dans les lignes ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bda3da-4a1b-421f-8d67-c02092353a38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-core:3.4.1`\n",
    "import $ivy.`org.apache.spark::spark-sql:3.4.1`\n",
    "import $ivy.`org.slf4j:slf4j-reload4j:2.0.6`\n",
    "\n",
    "import org.apache.logging.log4j.Level\n",
    "import org.apache.logging.log4j.core.config.Configurator\n",
    "\n",
    "// Avoid disturbing logs\n",
    "Configurator.setRootLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d204e6-16f4-4168-b811-8015c0051c05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialisation du contexte / de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134919ac-1908-4bb4-b1f9-366b77c20e6a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.rdd._\n",
    "\n",
    "// Dans ce block, nous configurons et créeons une session Spark\n",
    "val spark = {\n",
    "  // La ligne ci-dessous est spécifique à ces notebooks.\n",
    "  // Normalement, une session Spark est créée en commençant par `SparkSession.builder()...`\n",
    "  NotebookSparkSession.builder()\n",
    "    // Cette ligne est spécifique à Almond et permet de configurer l'affichage des barres de progression\n",
    "    .progress(enable = true, keep = true, useBars = false)\n",
    "    // Cette ligne permet d'indiquer à Spark la configuration du master.\n",
    "    // Ici, nous lançons Spark en local uniquement.\n",
    "    // `*` indique que nous utilisons tous les cores au niveau du CPU.\n",
    "    .master(\"local[*]\")\n",
    "    // La ligne ci-dessous sert à donner un nom à votre application\n",
    "    // Ce nom apparaîtra notamment dans la Spark UI\n",
    "    .appName(\"Spark RDD - Introduction\")\n",
    "    .getOrCreate()\n",
    "}\n",
    "\n",
    "// Permet de fournir des fonctions qui facilitent l'utilisation de Spark (en particulier Spark SQL).\n",
    "import spark.implicits._\n",
    "\n",
    "// Les lignes ci-dessous fournissent des éléments supplémentaires pour rendre l'affichage plus confortable\n",
    "import $file.^.internal.spark_helper\n",
    "import spark_helper.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710e6da-83d3-452b-99f6-20f4186285a3",
   "metadata": {},
   "source": [
    "### Nous allons maintenant récupérer le SparkContext\n",
    "\n",
    "Les lignes au-dessus permettent de récupérer ce qui s'appelle un SparkSession. Un SparkSession représente la configuration et le runtime utilisé par SparkSQL. Plus exactement, il s'agit un NotebookSparkSession. Il s'agit d'un wrapper mise en place par le noyau Almond, afin de fournir notamment des barres de progression pour les différents calculs que vous allez effectuer. À partir d'un SparkSession, il est possible de récupérer le SparkContext.\n",
    "\n",
    "Il est à noter qu'habituellement, il est possible de créer un SparkContext directement sans passer par un SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8940b21-6d30-4199-a472-31119e031b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Récupération du SparkContext\n",
    "val sparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06317b0b-3a7b-48c3-aa7a-9c7c0aa7847f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Lecture d'un fichier avec Spark Core\n",
    "\n",
    "Nous allons récupérer le fichier `orders.csv` et réaliser des analyses sur ce fichier.\n",
    "\n",
    "Commençons par afficher un extrait de son contenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f45923-c38a-4ba3-8606-7e0e4f4208e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%shell\n",
    "\n",
    "cat orders.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d44eb1-ea51-4da1-b894-f4c1356b38ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "Nous allons utiliser Spark pour récupérer le contenue de fichier.\n",
    "\n",
    "Dans la cellule ci-dessous, vous allez utiliser la méthode `.textFile()` ([lien](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/SparkContext.html#textFile(path:String,minPartitions:Int):org.apache.spark.rdd.RDD[String])) sur le `sparkContext` pour récupérer le contenu du fichier `orders.csv`, situé dans le même répertoire que ce notebook.\n",
    "\n",
    "La méthode `.textFile()` va récupérer le fichier, dont le nom est passé en paramètre. Comme, on demande de voir le fichier comme un fichier texte (`text` dans le nom de la méthode), il sera découpé ligne par ligne par cette méthode. Le résultat est un `RDD[String]`, où il faut voir le type `RDD` comme une collection. Le résultat est donc vu comme une collection de lignes, provenant du fichier `orders.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d92fd9-403c-4ad6-8197-76499504a2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val rawData: RDD[String] = ???\n",
    "\n",
    "rawData.showHTML(title=\"Extrait de rawData\", limit=10, truncate=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebcb2a-9c83-4fa7-84e8-0fb2cf087198",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Nous avons réussi à charger le fichier CSV avec Spark et nous avons pu en afficher une partie. Cependant, nous pouvons remarquer que la première ligne, qui représente les en-têtes de colonne, devrait être retirée du dataset pour pouvoir être exploitable. C'est que nous allons faire dans la cellule ci-dessous.\n",
    "\n",
    "Si vous allez voir dans la Spark UI, vous verrez qu'un job apparaît. Ce job est appelé `showHTML`. Nous pouvons voir son nom un peu plus haut dans ce notebook, juste au-dessus de la table.\n",
    "\n",
    "`.showHTML()` est une facilité proposée dans le cadre de cette formation. Normalement, pour récupérer les données d'un RDD, il faut utiliser des méthodes comme `.collect()` ou `.take(n)`.\n",
    "\n",
    "Une autre façon d'avoir un affichage formaté est d'utiliser le _magic hook_ `%%data`, toujours proposé dans le cadre de cette formation. Une cellule commençant par `%%data` implique que la dernière expression soit un _array_, une collection, ou un RDD. Une telle cellule ajoute un appel à `showHTML()` sur la dernière expression. Il est possible d'ajouter 2 paramètres optionnels à `%%data`: `limit` avec le nombre de lignes à afficher, `truncate` avec le nombre de caractères maximal à afficher par cellule.\n",
    "\n",
    "Voici un exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a7fe3-4683-481d-9598-8d483d0e9c91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%data limit=10,truncate=120\n",
    "\n",
    "rawData.map(_.split(\",\").toList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d43f4-8332-4721-a5bb-35e638ab2a5b",
   "metadata": {},
   "source": [
    "Nous allons retirer l'en-tête de notre jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fa5721-4907-4232-95d3-e2f5dbd7241c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Récupération de la première ligne (en-têtes)\n",
    "val header: String = rawData.first()\n",
    "\n",
    "// Récupération des lignes du dataset, sauf celle contenant les en-têtes de colonne\n",
    "// et séparation des colonnes\n",
    "val data: RDD[String] =\n",
    "  rawData\n",
    "    .filter(line => line != header)\n",
    "\n",
    "data.showHTML(title=\"Extrait de data\", limit=10, truncate=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80113bef-d0a0-4fe1-a3ef-1b2f388d74bd",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Cette fois, nous avons 2 jobs qui ont été lancés : un job pour `first()` et un job pour `showHTML`. Cela se voit aussi au niveau des barres de progression, mais aussi dans la Spark UI. Dans le cadre du second job, dans la Spark UI, nous pouvons voir qu'il est composé de deux opérations : une pour récupérer le contenu du fichier et une autre pour filtrer l'en-tête des colonnes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d55243-f19f-431e-bf55-1922fed442a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Première requête : nombre total de ventes\n",
    "\n",
    "Maintenant que nous avons chargé un fichier, nous pouvoir commencer à y effectuer des traitements.\n",
    "\n",
    "Dans la cellule ci-dessous, utilisez la méthodes `.count()` ([lien](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html#count():Long)) sur le RDD `data` afin d'avoir le nombre total de ventes contenu dans le fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52961341-87c8-4978-b3d5-ffa04863f0d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val totalSales: Long = ???\n",
    "\n",
    "s\"Nombre total de ventes: $totalSales\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b894f-ec62-4d0e-9ec1-8cba3d832823",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Une différence subtile apparaît dans la barre de progression : nous voyons l'indication `2 / 2`. Ceci signifie que 2 tâches ont été utilisées pour exécuter le job. Une tâche représente une opération effectuée par un exécuteur sur une partition.\n",
    "\n",
    "Jusque-là, seule une tâche avait été utilisée. Dans la mesure où nous avions à afficher seulement une partie des données, seule une seule partition était exploitée.\n",
    "\n",
    "Dans le cas du `count`, nous avons besoin d'accéder à toutes les partitions utilisées par le contenu du fichier `orders.csv`. Or ce fichier est éparpillé sur 2 partitions.\n",
    "\n",
    "L'expression ci-dessous va nous en convaincre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c932192-9e9a-491a-8277-33c5679f387f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c745505-4c8b-428d-b387-45675f3a97a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conversion\n",
    "\n",
    "Avoir un RDD contenant les lignes d'un fichier sous forme de chaîne de caractères, ce n'est pas très pratique. Il nous faut convertir ces lignes dans une structure plus aisée à exploiter.\n",
    "\n",
    "Nous allons créer la structure `Order`, qui va nous permettre de convertir les lignes dans un format plus simple à manipuler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c11e1-3d9c-497e-b158-52802a454d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import java.time._\n",
    "import java.time.format._\n",
    "\n",
    "case class Order(\n",
    "  id:        String,\n",
    "  clientId:  String,\n",
    "  timestamp: LocalDateTime,\n",
    "  product:   String,\n",
    "  price:     Double\n",
    ")\n",
    "\n",
    "def toLocalDateTime(field: String): LocalDateTime =\n",
    "  LocalDateTime.parse(\n",
    "    field,\n",
    "    DateTimeFormatter.ISO_LOCAL_DATE_TIME\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119a5b02-68c2-491a-9ed6-8636940acbcd",
   "metadata": {},
   "source": [
    "Nous allons maintenant convertir notre `RDD[String]` en `RDD[Order]`.\n",
    "\n",
    "Pour cela, nous allons utiliser la fonction `.map()` ([lien](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html#count():Long)) sur le RDD `data`. `map` est une opération qui prend en paramètre une fonction et qui applique cette fonction sur chaque élément contenu dans le RDD.\n",
    "\n",
    "Mais avant cela, nous allons définir la fonction qui va permettre de convertir une ligne est `Order`.\n",
    "\n",
    "Note:\n",
    " * L'accès à un case `i` dans un tableau `a` (Array) se fait de la manière suivante : `a(i)`\n",
    " * Pour convertir une chaîne en LocalDateTime, nous allons utiliser la fonction `toLocalDateTime` définie plus haut\n",
    " * Pour convertir une chaîne une chaîne `s` en Double, il suffit de faire : `s.toDouble`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7aca31-e610-4ace-8445-44e3c0fd1d43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lineToOrder(line: String): Order = {\n",
    "  val fields = line.split(\",\")\n",
    "  Order(\n",
    "    id = fields(0),\n",
    "    clientId = fields(1),\n",
    "    timestamp = toLocalDateTime(fields(2)),\n",
    "    product = fields(3),\n",
    "    price = fields(4).toDouble,\n",
    "  )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3925f54-a8c0-4b11-9964-6a8eeb3173ff",
   "metadata": {},
   "source": [
    "Dans la cellule ci-dessous, nous allons utiliser la méthode `.map()` sur le RDD `data`.\n",
    "\n",
    "Note : en Scala, pour représenter une fonction (ou _lambda expression_) qui prend un paramètre `s` de type String et applique une fonction `f` dessus, il suffit d'écrire : `(s: String) => f(s)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49c37d-6697-4b9d-8165-754c94629d39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val orders: RDD[Order] = ???\n",
    "\n",
    "orders.showHTML(title=\"Extrait de orders\", limit=10, truncate=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cbc33d-d1b8-4e72-b96b-78b1bd82bdb4",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "L'opération `.map()` permet d'appliquer une transformation à chaque ligne rencontrée dans le contexte du RDD. Nous n'avons plus un `RDD[String]`, mais un `RDD[Order]`. Une telle structure sera plus exploitable avec (à nouveau) l'opération `.map()`, mais aussi avec d'autres opérations applicables sur les RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adcbd78-fa69-4a53-8d5e-b7f0fe8d497f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Trouvez le produit le plus vendu (ID du produit et quantité totale vendue)\n",
    "\n",
    "Trouver le produit le plus vendu va nécessiter :\n",
    " 1. d'organiser les données par produit\n",
    " 2. de compter ensuite le nombre de lignes pour chacun de ces produits\n",
    " 3. de trier les produits par rapport à ce décompte\n",
    " 4. de récupérer le produit avec le grand décompte."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47890b-e107-4352-8c1e-b8bc25b2bf26",
   "metadata": {},
   "source": [
    "### Organiser par produit\n",
    "Pour la première étape, nous allons utiliser la méthode `.keyBy()` ([lien](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html#keyBy[K](f:T=%3EK):org.apache.spark.rdd.RDD[(K,T)])) sur le RDD `orders`. Cette méthode prend en paramètre une fonction qui va indiquer la valeur utilisée pour la clé à partir d'un order (ici, ce sera le champ `product`). De cette méthode, il en sort un RDD de couple _nom du produit / order_. Ce type de RDD s'appelle un PairRDD et des méthodes spécifiques s'appliquent sur les RDD de cette catégorie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476a6278-bfa9-4e8e-b3a8-b6d6292ce457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val ordersByProduct: RDD[(String, Order)] = ???\n",
    "\n",
    "ordersByProduct.showHTML(limit=10, truncate=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67536028-d0d2-4c6d-9796-8c711595965f",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Chaque ligne se voit associer un nom de produit correspondant. Dans sa façon de le représenter, Spark utilise des tuples Scala, avec la notation `(a, b)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b59dba-4f19-42c9-be86-2829015c7c56",
   "metadata": {},
   "source": [
    "### Compter par produit (1)\n",
    "Nous allons compter le nombre de commandes par produit. Pour cela, nous allons d'abord associer à chaque ligne la valeur `1` pour pouvoir ensuite faire la somme de ces `1`.\n",
    "\n",
    "Pour cela, nous allons réutiliser l'opération `.map()` pour convertir le `RDD[(String, Order)]` en `RDD[(String, Int)]`, où `Int` apparaît, car nous allons remplacer Order par al valeur `1`.\n",
    "\n",
    "Note : comme pour l'opération `.map()`, nous partons d'un `RDD[(String, Order)]`, la fonction dans `map` aura une écriture un peu différente. On utilisera ce format `{ case (a, b) => /* do something with a and b */ }`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9277bcc5-70b6-4c13-8936-c4149fb28e78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val productAndOne: RDD[(String, Int)] = ???\n",
    "\n",
    "productAndOne.showHTML(truncate=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc42e2-8c7a-4593-84bb-d01b57d1fe53",
   "metadata": {},
   "source": [
    "### Compter par produit (2)\n",
    "Nous allons à présent compter par produit le nombre de `1`. Dans ce cadre, nous allons utiliser la méthode `.reduceByKey()` ([lien](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/PairRDDFunctions.html#reduceByKey(func:(V,V)=%3EV):org.apache.spark.rdd.RDD[(K,V)])).\n",
    "\n",
    "Cette méthode récupère les éléments un par un du `RDD[(String, Int)]`. Pour chaque élément, la partie String (nom du produit) va servir de clé, sachant qu'un résultat est généré par clé. La partie Int (qui vaut `1`) est utilisé dans la fonction passée en paramètre de `.reduceByKey()`.\n",
    "\n",
    "Ainsi, lorsqu'une nouvelle donnée est récupérée du RDD, sa valeur est passée en paramètre de la fonction dans `reduceByKey`. La valeur est alors agrégée avec un résultat intermédiaire. Le résultat de cette agrégation devient le nouveau résultat intermédiaire qui est utilisé pour l'élément suivant du RDD (pour la même clé).\n",
    "\n",
    "La fonction passée en paramètre de `reduceByKey` est fonction qui ressemble à `(a, b) => f(a, b)`, sachant que `a`, `b` et le résultat de l'agrégation de `a` et de `b` doivent tous avoir le même type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77dcc04-1d5c-4e0f-bea5-ceff704fe2d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val productCount: RDD[(String, Int)] = ???\n",
    "\n",
    "productCount.showHTML(truncate=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01f4d0-c9de-476b-b8a8-edb8af3b2f44",
   "metadata": {},
   "source": [
    "**Ce qu'il faut voir**\n",
    "\n",
    "Nous voyons à présent 3 barres de progression pour un seul job (showHTML).\n",
    "\n",
    "Sachant que nous avons besoin maintenant de toutes les données pour compter, les opérations que nous avons mises en place depuis la lecture du fichier jusqu'au `map` vont être exécutées sur les deux partitions contenant les données du fichier. Ce qui donne 2 tâches à ce niveau. Puis, le `reduceByKey` nécessite une étape préalable consistant à mettre les données ayant la même clé au niveau de la même unité de travail, au niveau du même exécuteur. Ceci se traduit par un échange de données (_shuffle_) et donc la mise place d'un nouveau _stage_, un pour chaque partition. C'est la raison pour laquelle nous avons 2 barres de progression de plus.\n",
    "\n",
    "Dans Spark UI, nous pouvons voir apparaître 2 nouveaux jobs showHTML. Sachant que pour le premier, l'ensemble des opérations sont faites au complet. Et pour le second job, certains _stages_ et tâches sont passées, car déjà effectuées dans le cadre du premier job.\n",
    "\n",
    "Note : lorsqu'il y a du _shuffle_, Spark doit générer les données avant de pouvoir les échanger. Cette opération tend à remplir la mémoire et les disques (au travers de la génération de fichiers) associés aux exécuteurs. C'est aussi ce qui fait que le _shuffle_ est coûteux. Les fichiers générés sont conservés jusqu'à ce que le RDD associé ne soit plus utilisé."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b50ffe-6df5-4857-b70a-dfe8e0c99655",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tri par décompte\n",
    "Nous allons maintenant trier les produits selon leur quantité vendue. Pour cela, nous utiliserons la fonction `.sortBy()` ([lien](https://spark.apache.org/docs/latest/api/scala/org/apache/spark/rdd/RDD.html#sortBy[K](f:T=%3EK,ascending:Boolean,numPartitions:Int)(implicitord:Ordering[K],implicitctag:scala.reflect.ClassTag[K]):org.apache.spark.rdd.RDD[T])). Cette fonction prend en paramètre une fonction qui à partir d'un élément du RDD extrait une valeur qui va servir à trier ces éléments entre eux. Un deuxième paramètre (_ascending_) optionel indique l'ordre dans lequel sont triés les éléments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d85a36-7363-4d49-8b29-b574a80d02d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val productSortedByCount: RDD[(String, Int)] = ???\n",
    "\n",
    "productSortedByCount.showHTML(truncate=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c59a5b5-33af-41a5-aca6-5b0e567b876c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.13",
   "language": "scala",
   "name": "scala213"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
